================Parameters Settings=================
Parameters:	Namespace(num_shots=1, position='all', params=['q', 'k', 'v'], r=6, alpha=1, dropout_rate=0.25, log_name='reviewer_1', encoder='both', visual='VPT', vptNumTokens=4, ctx_init='a photo of a', text='CoOp', textNumTokens=4, optimizer='adam', l2_reg=0.0, epochs=1, lr=0.0005, momentum=0.9, resume_dict=None, code_path='/home/user/Code/DePro', dataset_path='/data/UCDR/data', dataset='DomainNet', is_eccv_split=1, clip_backbone='ViT-B/32', CLS_NUM_TOKENS=300, DOM_NUM_TOKENS=5, debug_mode=0, dropout=0.5, seen_domain='quickdraw', holdout_domain='sketch', gallery_domain='real', include_auxillary_domains=1, udcdr=0, image_size=224, seed=0, batch_size=50, num_workers=6, early_stop=2, log_interval=100)
================Training Settings=================
lr = 0.0005
batch_size = 50
==================================================
Residual Attention Block 0: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 1: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 2: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 3: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 4: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 5: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 6: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 7: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 8: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 9: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 10: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 11: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 0: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 1: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 2: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 3: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 4: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 5: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 6: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 7: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 8: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 9: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 10: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 11: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
======== Context-aware Simulator Learning Setup========
visual.transformer.resblocks.0.attn.q_proj.w_lora_A
visual.transformer.resblocks.0.attn.q_proj.w_lora_B
visual.transformer.resblocks.0.attn.k_proj.w_lora_A
visual.transformer.resblocks.0.attn.k_proj.w_lora_B
visual.transformer.resblocks.0.attn.v_proj.w_lora_A
visual.transformer.resblocks.0.attn.v_proj.w_lora_B
visual.transformer.resblocks.1.attn.q_proj.w_lora_A
visual.transformer.resblocks.1.attn.q_proj.w_lora_B
visual.transformer.resblocks.1.attn.k_proj.w_lora_A
visual.transformer.resblocks.1.attn.k_proj.w_lora_B
visual.transformer.resblocks.1.attn.v_proj.w_lora_A
visual.transformer.resblocks.1.attn.v_proj.w_lora_B
visual.transformer.resblocks.2.attn.q_proj.w_lora_A
visual.transformer.resblocks.2.attn.q_proj.w_lora_B
visual.transformer.resblocks.2.attn.k_proj.w_lora_A
visual.transformer.resblocks.2.attn.k_proj.w_lora_B
visual.transformer.resblocks.2.attn.v_proj.w_lora_A
visual.transformer.resblocks.2.attn.v_proj.w_lora_B
visual.transformer.resblocks.3.attn.q_proj.w_lora_A
visual.transformer.resblocks.3.attn.q_proj.w_lora_B
visual.transformer.resblocks.3.attn.k_proj.w_lora_A
visual.transformer.resblocks.3.attn.k_proj.w_lora_B
visual.transformer.resblocks.3.attn.v_proj.w_lora_A
visual.transformer.resblocks.3.attn.v_proj.w_lora_B
visual.transformer.resblocks.4.attn.q_proj.w_lora_A
visual.transformer.resblocks.4.attn.q_proj.w_lora_B
visual.transformer.resblocks.4.attn.k_proj.w_lora_A
visual.transformer.resblocks.4.attn.k_proj.w_lora_B
visual.transformer.resblocks.4.attn.v_proj.w_lora_A
visual.transformer.resblocks.4.attn.v_proj.w_lora_B
visual.transformer.resblocks.5.attn.q_proj.w_lora_A
visual.transformer.resblocks.5.attn.q_proj.w_lora_B
visual.transformer.resblocks.5.attn.k_proj.w_lora_A
visual.transformer.resblocks.5.attn.k_proj.w_lora_B
visual.transformer.resblocks.5.attn.v_proj.w_lora_A
visual.transformer.resblocks.5.attn.v_proj.w_lora_B
visual.transformer.resblocks.6.attn.q_proj.w_lora_A
visual.transformer.resblocks.6.attn.q_proj.w_lora_B
visual.transformer.resblocks.6.attn.k_proj.w_lora_A
visual.transformer.resblocks.6.attn.k_proj.w_lora_B
visual.transformer.resblocks.6.attn.v_proj.w_lora_A
visual.transformer.resblocks.6.attn.v_proj.w_lora_B
visual.transformer.resblocks.7.attn.q_proj.w_lora_A
visual.transformer.resblocks.7.attn.q_proj.w_lora_B
visual.transformer.resblocks.7.attn.k_proj.w_lora_A
visual.transformer.resblocks.7.attn.k_proj.w_lora_B
visual.transformer.resblocks.7.attn.v_proj.w_lora_A
visual.transformer.resblocks.7.attn.v_proj.w_lora_B
visual.transformer.resblocks.8.attn.q_proj.w_lora_A
visual.transformer.resblocks.8.attn.q_proj.w_lora_B
visual.transformer.resblocks.8.attn.k_proj.w_lora_A
visual.transformer.resblocks.8.attn.k_proj.w_lora_B
visual.transformer.resblocks.8.attn.v_proj.w_lora_A
visual.transformer.resblocks.8.attn.v_proj.w_lora_B
visual.transformer.resblocks.9.attn.q_proj.w_lora_A
visual.transformer.resblocks.9.attn.q_proj.w_lora_B
visual.transformer.resblocks.9.attn.k_proj.w_lora_A
visual.transformer.resblocks.9.attn.k_proj.w_lora_B
visual.transformer.resblocks.9.attn.v_proj.w_lora_A
visual.transformer.resblocks.9.attn.v_proj.w_lora_B
visual.transformer.resblocks.10.attn.q_proj.w_lora_A
visual.transformer.resblocks.10.attn.q_proj.w_lora_B
visual.transformer.resblocks.10.attn.k_proj.w_lora_A
visual.transformer.resblocks.10.attn.k_proj.w_lora_B
visual.transformer.resblocks.10.attn.v_proj.w_lora_A
visual.transformer.resblocks.10.attn.v_proj.w_lora_B
visual.transformer.resblocks.11.attn.q_proj.w_lora_A
visual.transformer.resblocks.11.attn.q_proj.w_lora_B
visual.transformer.resblocks.11.attn.k_proj.w_lora_A
visual.transformer.resblocks.11.attn.k_proj.w_lora_B
visual.transformer.resblocks.11.attn.v_proj.w_lora_A
visual.transformer.resblocks.11.attn.v_proj.w_lora_B
transformer.resblocks.0.attn.q_proj.w_lora_A
transformer.resblocks.0.attn.q_proj.w_lora_B
transformer.resblocks.0.attn.k_proj.w_lora_A
transformer.resblocks.0.attn.k_proj.w_lora_B
transformer.resblocks.0.attn.v_proj.w_lora_A
transformer.resblocks.0.attn.v_proj.w_lora_B
transformer.resblocks.1.attn.q_proj.w_lora_A
transformer.resblocks.1.attn.q_proj.w_lora_B
transformer.resblocks.1.attn.k_proj.w_lora_A
transformer.resblocks.1.attn.k_proj.w_lora_B
transformer.resblocks.1.attn.v_proj.w_lora_A
transformer.resblocks.1.attn.v_proj.w_lora_B
transformer.resblocks.2.attn.q_proj.w_lora_A
transformer.resblocks.2.attn.q_proj.w_lora_B
transformer.resblocks.2.attn.k_proj.w_lora_A
transformer.resblocks.2.attn.k_proj.w_lora_B
transformer.resblocks.2.attn.v_proj.w_lora_A
transformer.resblocks.2.attn.v_proj.w_lora_B
transformer.resblocks.3.attn.q_proj.w_lora_A
transformer.resblocks.3.attn.q_proj.w_lora_B
transformer.resblocks.3.attn.k_proj.w_lora_A
transformer.resblocks.3.attn.k_proj.w_lora_B
transformer.resblocks.3.attn.v_proj.w_lora_A
transformer.resblocks.3.attn.v_proj.w_lora_B
transformer.resblocks.4.attn.q_proj.w_lora_A
transformer.resblocks.4.attn.q_proj.w_lora_B
transformer.resblocks.4.attn.k_proj.w_lora_A
transformer.resblocks.4.attn.k_proj.w_lora_B
transformer.resblocks.4.attn.v_proj.w_lora_A
transformer.resblocks.4.attn.v_proj.w_lora_B
transformer.resblocks.5.attn.q_proj.w_lora_A
transformer.resblocks.5.attn.q_proj.w_lora_B
transformer.resblocks.5.attn.k_proj.w_lora_A
transformer.resblocks.5.attn.k_proj.w_lora_B
transformer.resblocks.5.attn.v_proj.w_lora_A
transformer.resblocks.5.attn.v_proj.w_lora_B
transformer.resblocks.6.attn.q_proj.w_lora_A
transformer.resblocks.6.attn.q_proj.w_lora_B
transformer.resblocks.6.attn.k_proj.w_lora_A
transformer.resblocks.6.attn.k_proj.w_lora_B
transformer.resblocks.6.attn.v_proj.w_lora_A
transformer.resblocks.6.attn.v_proj.w_lora_B
transformer.resblocks.7.attn.q_proj.w_lora_A
transformer.resblocks.7.attn.q_proj.w_lora_B
transformer.resblocks.7.attn.k_proj.w_lora_A
transformer.resblocks.7.attn.k_proj.w_lora_B
transformer.resblocks.7.attn.v_proj.w_lora_A
transformer.resblocks.7.attn.v_proj.w_lora_B
transformer.resblocks.8.attn.q_proj.w_lora_A
transformer.resblocks.8.attn.q_proj.w_lora_B
transformer.resblocks.8.attn.k_proj.w_lora_A
transformer.resblocks.8.attn.k_proj.w_lora_B
transformer.resblocks.8.attn.v_proj.w_lora_A
transformer.resblocks.8.attn.v_proj.w_lora_B
transformer.resblocks.9.attn.q_proj.w_lora_A
transformer.resblocks.9.attn.q_proj.w_lora_B
transformer.resblocks.9.attn.k_proj.w_lora_A
transformer.resblocks.9.attn.k_proj.w_lora_B
transformer.resblocks.9.attn.v_proj.w_lora_A
transformer.resblocks.9.attn.v_proj.w_lora_B
transformer.resblocks.10.attn.q_proj.w_lora_A
transformer.resblocks.10.attn.q_proj.w_lora_B
transformer.resblocks.10.attn.k_proj.w_lora_A
transformer.resblocks.10.attn.k_proj.w_lora_B
transformer.resblocks.10.attn.v_proj.w_lora_A
transformer.resblocks.10.attn.v_proj.w_lora_B
transformer.resblocks.11.attn.q_proj.w_lora_A
transformer.resblocks.11.attn.q_proj.w_lora_B
transformer.resblocks.11.attn.k_proj.w_lora_A
transformer.resblocks.11.attn.k_proj.w_lora_B
transformer.resblocks.11.attn.v_proj.w_lora_A
transformer.resblocks.11.attn.v_proj.w_lora_B
tot=151277313, train = 552960
===============================================
epoch = [1/1]loss = {'net': 5.0657231092453, 'acc': 0.478}

***Validation***
udcdr == 0
Query:sketch; Gallery:real; Generalized:0

Query Emb Dim:torch.Size([9729, 512]); Gallery Emb Dim:torch.Size([24387, 512])
computing unormed situation
tensor(0.4769, device='cuda:1') tensor(0.4104, device='cuda:1')
computing normed situation
tensor(0.4731, device='cuda:1') tensor(0.3997, device='cuda:1')
un-norm situation:
learned: map: 0.47688040137290955, prec: 0.41044917702674866
norm situation:
learned: map: 0.4731268882751465, prec: 0.39973020553588867
Query:sketch; Gallery:real; Generalized:1

Query Emb Dim:torch.Size([9729, 512]); Gallery Emb Dim:torch.Size([36600, 512])
computing unormed situation
tensor(0.4189, device='cuda:1') tensor(0.3533, device='cuda:1')
computing normed situation
tensor(0.4168, device='cuda:1') tensor(0.3475, device='cuda:1')
un-norm situation:
learned: map: 0.4189462959766388, prec: 0.35334157943725586
norm situation:
learned: map: 0.4168136715888977, prec: 0.34745657444000244
udcdr == 1

Query Emb Dim:torch.Size([1883, 512]); Gallery Emb Dim:torch.Size([5857, 512])
computing unormed situation
tensor(0.5302, device='cuda:1') tensor(0.3264, device='cuda:1')
computing normed situation
tensor(0.5302, device='cuda:1') tensor(0.3080, device='cuda:1')
un-norm situation:
learned: map: 0.5301522016525269, prec: 0.3263993561267853
norm situation:
learned: map: 0.5302435159683228, prec: 0.307981938123703
Epoch Time:2m28s lr:0.0005000 mAP:0.4168 prec:0.3475


***Training and Validation complete***
================Parameters Settings=================
Parameters:	Namespace(num_shots=2, position='all', params=['q', 'k', 'v'], r=6, alpha=1, dropout_rate=0.25, log_name='reviewer_1', encoder='both', visual='VPT', vptNumTokens=4, ctx_init='a photo of a', text='CoOp', textNumTokens=4, optimizer='adam', l2_reg=0.0, epochs=1, lr=0.0005, momentum=0.9, resume_dict=None, code_path='/home/user/Code/DePro', dataset_path='/data/UCDR/data', dataset='DomainNet', is_eccv_split=1, clip_backbone='ViT-B/32', CLS_NUM_TOKENS=300, DOM_NUM_TOKENS=5, debug_mode=0, dropout=0.5, seen_domain='quickdraw', holdout_domain='sketch', gallery_domain='real', include_auxillary_domains=1, udcdr=0, image_size=224, seed=0, batch_size=50, num_workers=6, early_stop=2, log_interval=100)
================Training Settings=================
lr = 0.0005
batch_size = 50
==================================================
Residual Attention Block 0: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 1: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 2: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 3: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 4: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 5: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 6: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 7: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 8: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 9: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 10: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 11: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 0: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 1: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 2: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 3: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 4: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 5: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 6: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 7: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 8: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 9: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 10: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 11: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
======== Context-aware Simulator Learning Setup========
visual.transformer.resblocks.0.attn.q_proj.w_lora_A
visual.transformer.resblocks.0.attn.q_proj.w_lora_B
visual.transformer.resblocks.0.attn.k_proj.w_lora_A
visual.transformer.resblocks.0.attn.k_proj.w_lora_B
visual.transformer.resblocks.0.attn.v_proj.w_lora_A
visual.transformer.resblocks.0.attn.v_proj.w_lora_B
visual.transformer.resblocks.1.attn.q_proj.w_lora_A
visual.transformer.resblocks.1.attn.q_proj.w_lora_B
visual.transformer.resblocks.1.attn.k_proj.w_lora_A
visual.transformer.resblocks.1.attn.k_proj.w_lora_B
visual.transformer.resblocks.1.attn.v_proj.w_lora_A
visual.transformer.resblocks.1.attn.v_proj.w_lora_B
visual.transformer.resblocks.2.attn.q_proj.w_lora_A
visual.transformer.resblocks.2.attn.q_proj.w_lora_B
visual.transformer.resblocks.2.attn.k_proj.w_lora_A
visual.transformer.resblocks.2.attn.k_proj.w_lora_B
visual.transformer.resblocks.2.attn.v_proj.w_lora_A
visual.transformer.resblocks.2.attn.v_proj.w_lora_B
visual.transformer.resblocks.3.attn.q_proj.w_lora_A
visual.transformer.resblocks.3.attn.q_proj.w_lora_B
visual.transformer.resblocks.3.attn.k_proj.w_lora_A
visual.transformer.resblocks.3.attn.k_proj.w_lora_B
visual.transformer.resblocks.3.attn.v_proj.w_lora_A
visual.transformer.resblocks.3.attn.v_proj.w_lora_B
visual.transformer.resblocks.4.attn.q_proj.w_lora_A
visual.transformer.resblocks.4.attn.q_proj.w_lora_B
visual.transformer.resblocks.4.attn.k_proj.w_lora_A
visual.transformer.resblocks.4.attn.k_proj.w_lora_B
visual.transformer.resblocks.4.attn.v_proj.w_lora_A
visual.transformer.resblocks.4.attn.v_proj.w_lora_B
visual.transformer.resblocks.5.attn.q_proj.w_lora_A
visual.transformer.resblocks.5.attn.q_proj.w_lora_B
visual.transformer.resblocks.5.attn.k_proj.w_lora_A
visual.transformer.resblocks.5.attn.k_proj.w_lora_B
visual.transformer.resblocks.5.attn.v_proj.w_lora_A
visual.transformer.resblocks.5.attn.v_proj.w_lora_B
visual.transformer.resblocks.6.attn.q_proj.w_lora_A
visual.transformer.resblocks.6.attn.q_proj.w_lora_B
visual.transformer.resblocks.6.attn.k_proj.w_lora_A
visual.transformer.resblocks.6.attn.k_proj.w_lora_B
visual.transformer.resblocks.6.attn.v_proj.w_lora_A
visual.transformer.resblocks.6.attn.v_proj.w_lora_B
visual.transformer.resblocks.7.attn.q_proj.w_lora_A
visual.transformer.resblocks.7.attn.q_proj.w_lora_B
visual.transformer.resblocks.7.attn.k_proj.w_lora_A
visual.transformer.resblocks.7.attn.k_proj.w_lora_B
visual.transformer.resblocks.7.attn.v_proj.w_lora_A
visual.transformer.resblocks.7.attn.v_proj.w_lora_B
visual.transformer.resblocks.8.attn.q_proj.w_lora_A
visual.transformer.resblocks.8.attn.q_proj.w_lora_B
visual.transformer.resblocks.8.attn.k_proj.w_lora_A
visual.transformer.resblocks.8.attn.k_proj.w_lora_B
visual.transformer.resblocks.8.attn.v_proj.w_lora_A
visual.transformer.resblocks.8.attn.v_proj.w_lora_B
visual.transformer.resblocks.9.attn.q_proj.w_lora_A
visual.transformer.resblocks.9.attn.q_proj.w_lora_B
visual.transformer.resblocks.9.attn.k_proj.w_lora_A
visual.transformer.resblocks.9.attn.k_proj.w_lora_B
visual.transformer.resblocks.9.attn.v_proj.w_lora_A
visual.transformer.resblocks.9.attn.v_proj.w_lora_B
visual.transformer.resblocks.10.attn.q_proj.w_lora_A
visual.transformer.resblocks.10.attn.q_proj.w_lora_B
visual.transformer.resblocks.10.attn.k_proj.w_lora_A
visual.transformer.resblocks.10.attn.k_proj.w_lora_B
visual.transformer.resblocks.10.attn.v_proj.w_lora_A
visual.transformer.resblocks.10.attn.v_proj.w_lora_B
visual.transformer.resblocks.11.attn.q_proj.w_lora_A
visual.transformer.resblocks.11.attn.q_proj.w_lora_B
visual.transformer.resblocks.11.attn.k_proj.w_lora_A
visual.transformer.resblocks.11.attn.k_proj.w_lora_B
visual.transformer.resblocks.11.attn.v_proj.w_lora_A
visual.transformer.resblocks.11.attn.v_proj.w_lora_B
transformer.resblocks.0.attn.q_proj.w_lora_A
transformer.resblocks.0.attn.q_proj.w_lora_B
transformer.resblocks.0.attn.k_proj.w_lora_A
transformer.resblocks.0.attn.k_proj.w_lora_B
transformer.resblocks.0.attn.v_proj.w_lora_A
transformer.resblocks.0.attn.v_proj.w_lora_B
transformer.resblocks.1.attn.q_proj.w_lora_A
transformer.resblocks.1.attn.q_proj.w_lora_B
transformer.resblocks.1.attn.k_proj.w_lora_A
transformer.resblocks.1.attn.k_proj.w_lora_B
transformer.resblocks.1.attn.v_proj.w_lora_A
transformer.resblocks.1.attn.v_proj.w_lora_B
transformer.resblocks.2.attn.q_proj.w_lora_A
transformer.resblocks.2.attn.q_proj.w_lora_B
transformer.resblocks.2.attn.k_proj.w_lora_A
transformer.resblocks.2.attn.k_proj.w_lora_B
transformer.resblocks.2.attn.v_proj.w_lora_A
transformer.resblocks.2.attn.v_proj.w_lora_B
transformer.resblocks.3.attn.q_proj.w_lora_A
transformer.resblocks.3.attn.q_proj.w_lora_B
transformer.resblocks.3.attn.k_proj.w_lora_A
transformer.resblocks.3.attn.k_proj.w_lora_B
transformer.resblocks.3.attn.v_proj.w_lora_A
transformer.resblocks.3.attn.v_proj.w_lora_B
transformer.resblocks.4.attn.q_proj.w_lora_A
transformer.resblocks.4.attn.q_proj.w_lora_B
transformer.resblocks.4.attn.k_proj.w_lora_A
transformer.resblocks.4.attn.k_proj.w_lora_B
transformer.resblocks.4.attn.v_proj.w_lora_A
transformer.resblocks.4.attn.v_proj.w_lora_B
transformer.resblocks.5.attn.q_proj.w_lora_A
transformer.resblocks.5.attn.q_proj.w_lora_B
transformer.resblocks.5.attn.k_proj.w_lora_A
transformer.resblocks.5.attn.k_proj.w_lora_B
transformer.resblocks.5.attn.v_proj.w_lora_A
transformer.resblocks.5.attn.v_proj.w_lora_B
transformer.resblocks.6.attn.q_proj.w_lora_A
transformer.resblocks.6.attn.q_proj.w_lora_B
transformer.resblocks.6.attn.k_proj.w_lora_A
transformer.resblocks.6.attn.k_proj.w_lora_B
transformer.resblocks.6.attn.v_proj.w_lora_A
transformer.resblocks.6.attn.v_proj.w_lora_B
transformer.resblocks.7.attn.q_proj.w_lora_A
transformer.resblocks.7.attn.q_proj.w_lora_B
transformer.resblocks.7.attn.k_proj.w_lora_A
transformer.resblocks.7.attn.k_proj.w_lora_B
transformer.resblocks.7.attn.v_proj.w_lora_A
transformer.resblocks.7.attn.v_proj.w_lora_B
transformer.resblocks.8.attn.q_proj.w_lora_A
transformer.resblocks.8.attn.q_proj.w_lora_B
transformer.resblocks.8.attn.k_proj.w_lora_A
transformer.resblocks.8.attn.k_proj.w_lora_B
transformer.resblocks.8.attn.v_proj.w_lora_A
transformer.resblocks.8.attn.v_proj.w_lora_B
transformer.resblocks.9.attn.q_proj.w_lora_A
transformer.resblocks.9.attn.q_proj.w_lora_B
transformer.resblocks.9.attn.k_proj.w_lora_A
transformer.resblocks.9.attn.k_proj.w_lora_B
transformer.resblocks.9.attn.v_proj.w_lora_A
transformer.resblocks.9.attn.v_proj.w_lora_B
transformer.resblocks.10.attn.q_proj.w_lora_A
transformer.resblocks.10.attn.q_proj.w_lora_B
transformer.resblocks.10.attn.k_proj.w_lora_A
transformer.resblocks.10.attn.k_proj.w_lora_B
transformer.resblocks.10.attn.v_proj.w_lora_A
transformer.resblocks.10.attn.v_proj.w_lora_B
transformer.resblocks.11.attn.q_proj.w_lora_A
transformer.resblocks.11.attn.q_proj.w_lora_B
transformer.resblocks.11.attn.k_proj.w_lora_A
transformer.resblocks.11.attn.k_proj.w_lora_B
transformer.resblocks.11.attn.v_proj.w_lora_A
transformer.resblocks.11.attn.v_proj.w_lora_B
tot=151277313, train = 552960
===============================================
epoch = [1/1]loss = {'net': 9.901124668121337, 'acc': 0.473}

***Validation***
udcdr == 0
Query:sketch; Gallery:real; Generalized:0

Query Emb Dim:torch.Size([9729, 512]); Gallery Emb Dim:torch.Size([24387, 512])
computing unormed situation
tensor(0.4513, device='cuda:1') tensor(0.3772, device='cuda:1')
computing normed situation
tensor(0.5485, device='cuda:1') tensor(0.4923, device='cuda:1')
un-norm situation:
learned: map: 0.451340913772583, prec: 0.37720420956611633
norm situation:
learned: map: 0.5485414862632751, prec: 0.49231478571891785
Query:sketch; Gallery:real; Generalized:1

Query Emb Dim:torch.Size([9729, 512]); Gallery Emb Dim:torch.Size([36600, 512])
computing unormed situation
tensor(0.4013, device='cuda:1') tensor(0.3236, device='cuda:1')
computing normed situation
tensor(0.4871, device='cuda:1') tensor(0.4333, device='cuda:1')
un-norm situation:
learned: map: 0.4012559652328491, prec: 0.32361137866973877
norm situation:
learned: map: 0.4870559573173523, prec: 0.43333539366722107
udcdr == 1

Query Emb Dim:torch.Size([1883, 512]); Gallery Emb Dim:torch.Size([5857, 512])
computing unormed situation
tensor(0.5272, device='cuda:1') tensor(0.3384, device='cuda:1')
computing normed situation
tensor(0.6205, device='cuda:1') tensor(0.3923, device='cuda:1')
un-norm situation:
learned: map: 0.5271918177604675, prec: 0.33844664692878723
norm situation:
learned: map: 0.6204892992973328, prec: 0.3923340439796448
Epoch Time:2m33s lr:0.0005000 mAP:0.4871 prec:0.4333


***Training and Validation complete***
================Parameters Settings=================
Parameters:	Namespace(num_shots=4, position='all', params=['q', 'k', 'v'], r=6, alpha=1, dropout_rate=0.25, log_name='reviewer_1', encoder='both', visual='VPT', vptNumTokens=4, ctx_init='a photo of a', text='CoOp', textNumTokens=4, optimizer='adam', l2_reg=0.0, epochs=1, lr=0.0005, momentum=0.9, resume_dict=None, code_path='/home/user/Code/DePro', dataset_path='/data/UCDR/data', dataset='DomainNet', is_eccv_split=1, clip_backbone='ViT-B/32', CLS_NUM_TOKENS=300, DOM_NUM_TOKENS=5, debug_mode=0, dropout=0.5, seen_domain='quickdraw', holdout_domain='sketch', gallery_domain='real', include_auxillary_domains=1, udcdr=0, image_size=224, seed=0, batch_size=50, num_workers=6, early_stop=2, log_interval=100)
================Training Settings=================
lr = 0.0005
batch_size = 50
==================================================
Residual Attention Block 0: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 1: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 2: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 3: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 4: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 5: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 6: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 7: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 8: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 9: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 10: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 11: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 0: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 1: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 2: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 3: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 4: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 5: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 6: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 7: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 8: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 9: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 10: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 11: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
======== Context-aware Simulator Learning Setup========
visual.transformer.resblocks.0.attn.q_proj.w_lora_A
visual.transformer.resblocks.0.attn.q_proj.w_lora_B
visual.transformer.resblocks.0.attn.k_proj.w_lora_A
visual.transformer.resblocks.0.attn.k_proj.w_lora_B
visual.transformer.resblocks.0.attn.v_proj.w_lora_A
visual.transformer.resblocks.0.attn.v_proj.w_lora_B
visual.transformer.resblocks.1.attn.q_proj.w_lora_A
visual.transformer.resblocks.1.attn.q_proj.w_lora_B
visual.transformer.resblocks.1.attn.k_proj.w_lora_A
visual.transformer.resblocks.1.attn.k_proj.w_lora_B
visual.transformer.resblocks.1.attn.v_proj.w_lora_A
visual.transformer.resblocks.1.attn.v_proj.w_lora_B
visual.transformer.resblocks.2.attn.q_proj.w_lora_A
visual.transformer.resblocks.2.attn.q_proj.w_lora_B
visual.transformer.resblocks.2.attn.k_proj.w_lora_A
visual.transformer.resblocks.2.attn.k_proj.w_lora_B
visual.transformer.resblocks.2.attn.v_proj.w_lora_A
visual.transformer.resblocks.2.attn.v_proj.w_lora_B
visual.transformer.resblocks.3.attn.q_proj.w_lora_A
visual.transformer.resblocks.3.attn.q_proj.w_lora_B
visual.transformer.resblocks.3.attn.k_proj.w_lora_A
visual.transformer.resblocks.3.attn.k_proj.w_lora_B
visual.transformer.resblocks.3.attn.v_proj.w_lora_A
visual.transformer.resblocks.3.attn.v_proj.w_lora_B
visual.transformer.resblocks.4.attn.q_proj.w_lora_A
visual.transformer.resblocks.4.attn.q_proj.w_lora_B
visual.transformer.resblocks.4.attn.k_proj.w_lora_A
visual.transformer.resblocks.4.attn.k_proj.w_lora_B
visual.transformer.resblocks.4.attn.v_proj.w_lora_A
visual.transformer.resblocks.4.attn.v_proj.w_lora_B
visual.transformer.resblocks.5.attn.q_proj.w_lora_A
visual.transformer.resblocks.5.attn.q_proj.w_lora_B
visual.transformer.resblocks.5.attn.k_proj.w_lora_A
visual.transformer.resblocks.5.attn.k_proj.w_lora_B
visual.transformer.resblocks.5.attn.v_proj.w_lora_A
visual.transformer.resblocks.5.attn.v_proj.w_lora_B
visual.transformer.resblocks.6.attn.q_proj.w_lora_A
visual.transformer.resblocks.6.attn.q_proj.w_lora_B
visual.transformer.resblocks.6.attn.k_proj.w_lora_A
visual.transformer.resblocks.6.attn.k_proj.w_lora_B
visual.transformer.resblocks.6.attn.v_proj.w_lora_A
visual.transformer.resblocks.6.attn.v_proj.w_lora_B
visual.transformer.resblocks.7.attn.q_proj.w_lora_A
visual.transformer.resblocks.7.attn.q_proj.w_lora_B
visual.transformer.resblocks.7.attn.k_proj.w_lora_A
visual.transformer.resblocks.7.attn.k_proj.w_lora_B
visual.transformer.resblocks.7.attn.v_proj.w_lora_A
visual.transformer.resblocks.7.attn.v_proj.w_lora_B
visual.transformer.resblocks.8.attn.q_proj.w_lora_A
visual.transformer.resblocks.8.attn.q_proj.w_lora_B
visual.transformer.resblocks.8.attn.k_proj.w_lora_A
visual.transformer.resblocks.8.attn.k_proj.w_lora_B
visual.transformer.resblocks.8.attn.v_proj.w_lora_A
visual.transformer.resblocks.8.attn.v_proj.w_lora_B
visual.transformer.resblocks.9.attn.q_proj.w_lora_A
visual.transformer.resblocks.9.attn.q_proj.w_lora_B
visual.transformer.resblocks.9.attn.k_proj.w_lora_A
visual.transformer.resblocks.9.attn.k_proj.w_lora_B
visual.transformer.resblocks.9.attn.v_proj.w_lora_A
visual.transformer.resblocks.9.attn.v_proj.w_lora_B
visual.transformer.resblocks.10.attn.q_proj.w_lora_A
visual.transformer.resblocks.10.attn.q_proj.w_lora_B
visual.transformer.resblocks.10.attn.k_proj.w_lora_A
visual.transformer.resblocks.10.attn.k_proj.w_lora_B
visual.transformer.resblocks.10.attn.v_proj.w_lora_A
visual.transformer.resblocks.10.attn.v_proj.w_lora_B
visual.transformer.resblocks.11.attn.q_proj.w_lora_A
visual.transformer.resblocks.11.attn.q_proj.w_lora_B
visual.transformer.resblocks.11.attn.k_proj.w_lora_A
visual.transformer.resblocks.11.attn.k_proj.w_lora_B
visual.transformer.resblocks.11.attn.v_proj.w_lora_A
visual.transformer.resblocks.11.attn.v_proj.w_lora_B
transformer.resblocks.0.attn.q_proj.w_lora_A
transformer.resblocks.0.attn.q_proj.w_lora_B
transformer.resblocks.0.attn.k_proj.w_lora_A
transformer.resblocks.0.attn.k_proj.w_lora_B
transformer.resblocks.0.attn.v_proj.w_lora_A
transformer.resblocks.0.attn.v_proj.w_lora_B
transformer.resblocks.1.attn.q_proj.w_lora_A
transformer.resblocks.1.attn.q_proj.w_lora_B
transformer.resblocks.1.attn.k_proj.w_lora_A
transformer.resblocks.1.attn.k_proj.w_lora_B
transformer.resblocks.1.attn.v_proj.w_lora_A
transformer.resblocks.1.attn.v_proj.w_lora_B
transformer.resblocks.2.attn.q_proj.w_lora_A
transformer.resblocks.2.attn.q_proj.w_lora_B
transformer.resblocks.2.attn.k_proj.w_lora_A
transformer.resblocks.2.attn.k_proj.w_lora_B
transformer.resblocks.2.attn.v_proj.w_lora_A
transformer.resblocks.2.attn.v_proj.w_lora_B
transformer.resblocks.3.attn.q_proj.w_lora_A
transformer.resblocks.3.attn.q_proj.w_lora_B
transformer.resblocks.3.attn.k_proj.w_lora_A
transformer.resblocks.3.attn.k_proj.w_lora_B
transformer.resblocks.3.attn.v_proj.w_lora_A
transformer.resblocks.3.attn.v_proj.w_lora_B
transformer.resblocks.4.attn.q_proj.w_lora_A
transformer.resblocks.4.attn.q_proj.w_lora_B
transformer.resblocks.4.attn.k_proj.w_lora_A
transformer.resblocks.4.attn.k_proj.w_lora_B
transformer.resblocks.4.attn.v_proj.w_lora_A
transformer.resblocks.4.attn.v_proj.w_lora_B
transformer.resblocks.5.attn.q_proj.w_lora_A
transformer.resblocks.5.attn.q_proj.w_lora_B
transformer.resblocks.5.attn.k_proj.w_lora_A
transformer.resblocks.5.attn.k_proj.w_lora_B
transformer.resblocks.5.attn.v_proj.w_lora_A
transformer.resblocks.5.attn.v_proj.w_lora_B
transformer.resblocks.6.attn.q_proj.w_lora_A
transformer.resblocks.6.attn.q_proj.w_lora_B
transformer.resblocks.6.attn.k_proj.w_lora_A
transformer.resblocks.6.attn.k_proj.w_lora_B
transformer.resblocks.6.attn.v_proj.w_lora_A
transformer.resblocks.6.attn.v_proj.w_lora_B
transformer.resblocks.7.attn.q_proj.w_lora_A
transformer.resblocks.7.attn.q_proj.w_lora_B
transformer.resblocks.7.attn.k_proj.w_lora_A
transformer.resblocks.7.attn.k_proj.w_lora_B
transformer.resblocks.7.attn.v_proj.w_lora_A
transformer.resblocks.7.attn.v_proj.w_lora_B
transformer.resblocks.8.attn.q_proj.w_lora_A
transformer.resblocks.8.attn.q_proj.w_lora_B
transformer.resblocks.8.attn.k_proj.w_lora_A
transformer.resblocks.8.attn.k_proj.w_lora_B
transformer.resblocks.8.attn.v_proj.w_lora_A
transformer.resblocks.8.attn.v_proj.w_lora_B
transformer.resblocks.9.attn.q_proj.w_lora_A
transformer.resblocks.9.attn.q_proj.w_lora_B
transformer.resblocks.9.attn.k_proj.w_lora_A
transformer.resblocks.9.attn.k_proj.w_lora_B
transformer.resblocks.9.attn.v_proj.w_lora_A
transformer.resblocks.9.attn.v_proj.w_lora_B
transformer.resblocks.10.attn.q_proj.w_lora_A
transformer.resblocks.10.attn.q_proj.w_lora_B
transformer.resblocks.10.attn.k_proj.w_lora_A
transformer.resblocks.10.attn.k_proj.w_lora_B
transformer.resblocks.10.attn.v_proj.w_lora_A
transformer.resblocks.10.attn.v_proj.w_lora_B
transformer.resblocks.11.attn.q_proj.w_lora_A
transformer.resblocks.11.attn.q_proj.w_lora_B
transformer.resblocks.11.attn.k_proj.w_lora_A
transformer.resblocks.11.attn.k_proj.w_lora_B
transformer.resblocks.11.attn.v_proj.w_lora_A
transformer.resblocks.11.attn.v_proj.w_lora_B
tot=151277313, train = 552960
===============================================
[Train] Epoch: [1/1][100/120]  Time 0.165 (0.410)  cls 3.3407 (3.0152)  contrastive 1.0431 (6.0644)  loss 4.3838 (9.0797)  
epoch = [1/1]loss = {'net': 8.233327466249467, 'acc': 0.4615}

***Validation***
udcdr == 0
Query:sketch; Gallery:real; Generalized:0

Query Emb Dim:torch.Size([9729, 512]); Gallery Emb Dim:torch.Size([24387, 512])
computing unormed situation
tensor(0.3534, device='cuda:1') tensor(0.2357, device='cuda:1')
computing normed situation
tensor(0.4255, device='cuda:1') tensor(0.3252, device='cuda:1')
un-norm situation:
learned: map: 0.3533720076084137, prec: 0.235723614692688
norm situation:
learned: map: 0.42546185851097107, prec: 0.3251798748970032
Query:sketch; Gallery:real; Generalized:1

Query Emb Dim:torch.Size([9729, 512]); Gallery Emb Dim:torch.Size([36600, 512])
computing unormed situation
tensor(0.3188, device='cuda:1') tensor(0.2014, device='cuda:1')
computing normed situation
tensor(0.3786, device='cuda:1') tensor(0.2817, device='cuda:1')
un-norm situation:
learned: map: 0.3187858760356903, prec: 0.20140866935253143
norm situation:
learned: map: 0.3785940408706665, prec: 0.28171753883361816
udcdr == 1

Query Emb Dim:torch.Size([1883, 512]); Gallery Emb Dim:torch.Size([5857, 512])
computing unormed situation
tensor(0.4302, device='cuda:1') tensor(0.2222, device='cuda:1')
computing normed situation
tensor(0.5135, device='cuda:1') tensor(0.2968, device='cuda:1')
un-norm situation:
learned: map: 0.4302370250225067, prec: 0.22221188247203827
norm situation:
learned: map: 0.5134689807891846, prec: 0.29681095480918884
Epoch Time:3m11s lr:0.0005000 mAP:0.3786 prec:0.2817


***Training and Validation complete***
================Parameters Settings=================
Parameters:	Namespace(num_shots=8, position='all', params=['q', 'k', 'v'], r=6, alpha=1, dropout_rate=0.25, log_name='reviewer_1', encoder='both', visual='VPT', vptNumTokens=4, ctx_init='a photo of a', text='CoOp', textNumTokens=4, optimizer='adam', l2_reg=0.0, epochs=1, lr=0.0005, momentum=0.9, resume_dict=None, code_path='/home/user/Code/DePro', dataset_path='/data/UCDR/data', dataset='DomainNet', is_eccv_split=1, clip_backbone='ViT-B/32', CLS_NUM_TOKENS=300, DOM_NUM_TOKENS=5, debug_mode=0, dropout=0.5, seen_domain='quickdraw', holdout_domain='sketch', gallery_domain='real', include_auxillary_domains=1, udcdr=0, image_size=224, seed=0, batch_size=50, num_workers=6, early_stop=2, log_interval=100)
================Training Settings=================
lr = 0.0005
batch_size = 50
==================================================
Residual Attention Block 0: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 1: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 2: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 3: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 4: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 5: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 6: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 7: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 8: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 9: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 10: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 11: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 0: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 1: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 2: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 3: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 4: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 5: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 6: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 7: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 8: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 9: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 10: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 11: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
======== Context-aware Simulator Learning Setup========
visual.transformer.resblocks.0.attn.q_proj.w_lora_A
visual.transformer.resblocks.0.attn.q_proj.w_lora_B
visual.transformer.resblocks.0.attn.k_proj.w_lora_A
visual.transformer.resblocks.0.attn.k_proj.w_lora_B
visual.transformer.resblocks.0.attn.v_proj.w_lora_A
visual.transformer.resblocks.0.attn.v_proj.w_lora_B
visual.transformer.resblocks.1.attn.q_proj.w_lora_A
visual.transformer.resblocks.1.attn.q_proj.w_lora_B
visual.transformer.resblocks.1.attn.k_proj.w_lora_A
visual.transformer.resblocks.1.attn.k_proj.w_lora_B
visual.transformer.resblocks.1.attn.v_proj.w_lora_A
visual.transformer.resblocks.1.attn.v_proj.w_lora_B
visual.transformer.resblocks.2.attn.q_proj.w_lora_A
visual.transformer.resblocks.2.attn.q_proj.w_lora_B
visual.transformer.resblocks.2.attn.k_proj.w_lora_A
visual.transformer.resblocks.2.attn.k_proj.w_lora_B
visual.transformer.resblocks.2.attn.v_proj.w_lora_A
visual.transformer.resblocks.2.attn.v_proj.w_lora_B
visual.transformer.resblocks.3.attn.q_proj.w_lora_A
visual.transformer.resblocks.3.attn.q_proj.w_lora_B
visual.transformer.resblocks.3.attn.k_proj.w_lora_A
visual.transformer.resblocks.3.attn.k_proj.w_lora_B
visual.transformer.resblocks.3.attn.v_proj.w_lora_A
visual.transformer.resblocks.3.attn.v_proj.w_lora_B
visual.transformer.resblocks.4.attn.q_proj.w_lora_A
visual.transformer.resblocks.4.attn.q_proj.w_lora_B
visual.transformer.resblocks.4.attn.k_proj.w_lora_A
visual.transformer.resblocks.4.attn.k_proj.w_lora_B
visual.transformer.resblocks.4.attn.v_proj.w_lora_A
visual.transformer.resblocks.4.attn.v_proj.w_lora_B
visual.transformer.resblocks.5.attn.q_proj.w_lora_A
visual.transformer.resblocks.5.attn.q_proj.w_lora_B
visual.transformer.resblocks.5.attn.k_proj.w_lora_A
visual.transformer.resblocks.5.attn.k_proj.w_lora_B
visual.transformer.resblocks.5.attn.v_proj.w_lora_A
visual.transformer.resblocks.5.attn.v_proj.w_lora_B
visual.transformer.resblocks.6.attn.q_proj.w_lora_A
visual.transformer.resblocks.6.attn.q_proj.w_lora_B
visual.transformer.resblocks.6.attn.k_proj.w_lora_A
visual.transformer.resblocks.6.attn.k_proj.w_lora_B
visual.transformer.resblocks.6.attn.v_proj.w_lora_A
visual.transformer.resblocks.6.attn.v_proj.w_lora_B
visual.transformer.resblocks.7.attn.q_proj.w_lora_A
visual.transformer.resblocks.7.attn.q_proj.w_lora_B
visual.transformer.resblocks.7.attn.k_proj.w_lora_A
visual.transformer.resblocks.7.attn.k_proj.w_lora_B
visual.transformer.resblocks.7.attn.v_proj.w_lora_A
visual.transformer.resblocks.7.attn.v_proj.w_lora_B
visual.transformer.resblocks.8.attn.q_proj.w_lora_A
visual.transformer.resblocks.8.attn.q_proj.w_lora_B
visual.transformer.resblocks.8.attn.k_proj.w_lora_A
visual.transformer.resblocks.8.attn.k_proj.w_lora_B
visual.transformer.resblocks.8.attn.v_proj.w_lora_A
visual.transformer.resblocks.8.attn.v_proj.w_lora_B
visual.transformer.resblocks.9.attn.q_proj.w_lora_A
visual.transformer.resblocks.9.attn.q_proj.w_lora_B
visual.transformer.resblocks.9.attn.k_proj.w_lora_A
visual.transformer.resblocks.9.attn.k_proj.w_lora_B
visual.transformer.resblocks.9.attn.v_proj.w_lora_A
visual.transformer.resblocks.9.attn.v_proj.w_lora_B
visual.transformer.resblocks.10.attn.q_proj.w_lora_A
visual.transformer.resblocks.10.attn.q_proj.w_lora_B
visual.transformer.resblocks.10.attn.k_proj.w_lora_A
visual.transformer.resblocks.10.attn.k_proj.w_lora_B
visual.transformer.resblocks.10.attn.v_proj.w_lora_A
visual.transformer.resblocks.10.attn.v_proj.w_lora_B
visual.transformer.resblocks.11.attn.q_proj.w_lora_A
visual.transformer.resblocks.11.attn.q_proj.w_lora_B
visual.transformer.resblocks.11.attn.k_proj.w_lora_A
visual.transformer.resblocks.11.attn.k_proj.w_lora_B
visual.transformer.resblocks.11.attn.v_proj.w_lora_A
visual.transformer.resblocks.11.attn.v_proj.w_lora_B
transformer.resblocks.0.attn.q_proj.w_lora_A
transformer.resblocks.0.attn.q_proj.w_lora_B
transformer.resblocks.0.attn.k_proj.w_lora_A
transformer.resblocks.0.attn.k_proj.w_lora_B
transformer.resblocks.0.attn.v_proj.w_lora_A
transformer.resblocks.0.attn.v_proj.w_lora_B
transformer.resblocks.1.attn.q_proj.w_lora_A
transformer.resblocks.1.attn.q_proj.w_lora_B
transformer.resblocks.1.attn.k_proj.w_lora_A
transformer.resblocks.1.attn.k_proj.w_lora_B
transformer.resblocks.1.attn.v_proj.w_lora_A
transformer.resblocks.1.attn.v_proj.w_lora_B
transformer.resblocks.2.attn.q_proj.w_lora_A
transformer.resblocks.2.attn.q_proj.w_lora_B
transformer.resblocks.2.attn.k_proj.w_lora_A
transformer.resblocks.2.attn.k_proj.w_lora_B
transformer.resblocks.2.attn.v_proj.w_lora_A
transformer.resblocks.2.attn.v_proj.w_lora_B
transformer.resblocks.3.attn.q_proj.w_lora_A
transformer.resblocks.3.attn.q_proj.w_lora_B
transformer.resblocks.3.attn.k_proj.w_lora_A
transformer.resblocks.3.attn.k_proj.w_lora_B
transformer.resblocks.3.attn.v_proj.w_lora_A
transformer.resblocks.3.attn.v_proj.w_lora_B
transformer.resblocks.4.attn.q_proj.w_lora_A
transformer.resblocks.4.attn.q_proj.w_lora_B
transformer.resblocks.4.attn.k_proj.w_lora_A
transformer.resblocks.4.attn.k_proj.w_lora_B
transformer.resblocks.4.attn.v_proj.w_lora_A
transformer.resblocks.4.attn.v_proj.w_lora_B
transformer.resblocks.5.attn.q_proj.w_lora_A
transformer.resblocks.5.attn.q_proj.w_lora_B
transformer.resblocks.5.attn.k_proj.w_lora_A
transformer.resblocks.5.attn.k_proj.w_lora_B
transformer.resblocks.5.attn.v_proj.w_lora_A
transformer.resblocks.5.attn.v_proj.w_lora_B
transformer.resblocks.6.attn.q_proj.w_lora_A
transformer.resblocks.6.attn.q_proj.w_lora_B
transformer.resblocks.6.attn.k_proj.w_lora_A
transformer.resblocks.6.attn.k_proj.w_lora_B
transformer.resblocks.6.attn.v_proj.w_lora_A
transformer.resblocks.6.attn.v_proj.w_lora_B
transformer.resblocks.7.attn.q_proj.w_lora_A
transformer.resblocks.7.attn.q_proj.w_lora_B
transformer.resblocks.7.attn.k_proj.w_lora_A
transformer.resblocks.7.attn.k_proj.w_lora_B
transformer.resblocks.7.attn.v_proj.w_lora_A
transformer.resblocks.7.attn.v_proj.w_lora_B
transformer.resblocks.8.attn.q_proj.w_lora_A
transformer.resblocks.8.attn.q_proj.w_lora_B
transformer.resblocks.8.attn.k_proj.w_lora_A
transformer.resblocks.8.attn.k_proj.w_lora_B
transformer.resblocks.8.attn.v_proj.w_lora_A
transformer.resblocks.8.attn.v_proj.w_lora_B
transformer.resblocks.9.attn.q_proj.w_lora_A
transformer.resblocks.9.attn.q_proj.w_lora_B
transformer.resblocks.9.attn.k_proj.w_lora_A
transformer.resblocks.9.attn.k_proj.w_lora_B
transformer.resblocks.9.attn.v_proj.w_lora_A
transformer.resblocks.9.attn.v_proj.w_lora_B
transformer.resblocks.10.attn.q_proj.w_lora_A
transformer.resblocks.10.attn.q_proj.w_lora_B
transformer.resblocks.10.attn.k_proj.w_lora_A
transformer.resblocks.10.attn.k_proj.w_lora_B
transformer.resblocks.10.attn.v_proj.w_lora_A
transformer.resblocks.10.attn.v_proj.w_lora_B
transformer.resblocks.11.attn.q_proj.w_lora_A
transformer.resblocks.11.attn.q_proj.w_lora_B
transformer.resblocks.11.attn.k_proj.w_lora_A
transformer.resblocks.11.attn.k_proj.w_lora_B
transformer.resblocks.11.attn.v_proj.w_lora_A
transformer.resblocks.11.attn.v_proj.w_lora_B
tot=151277313, train = 552960
===============================================
[Train] Epoch: [1/1][100/240]  Time 0.165 (0.357)  cls 2.7060 (3.0759)  contrastive 1.4744 (5.9140)  loss 4.1804 (8.9899)  
[Train] Epoch: [1/1][200/240]  Time 0.164 (0.341)  cls 1.9943 (2.8454)  contrastive 1.4622 (3.4565)  loss 3.4565 (6.3019)  
epoch = [1/1]loss = {'net': 5.799971193571886, 'acc': 0.4696666666666667}

***Validation***
udcdr == 0
Query:sketch; Gallery:real; Generalized:0

Query Emb Dim:torch.Size([9729, 512]); Gallery Emb Dim:torch.Size([24387, 512])
computing unormed situation
tensor(0.4297, device='cuda:1') tensor(0.3183, device='cuda:1')
computing normed situation
tensor(0.4991, device='cuda:1') tensor(0.4091, device='cuda:1')
un-norm situation:
learned: map: 0.4296525716781616, prec: 0.3182942867279053
norm situation:
learned: map: 0.49914515018463135, prec: 0.40906569361686707
Query:sketch; Gallery:real; Generalized:1

Query Emb Dim:torch.Size([9729, 512]); Gallery Emb Dim:torch.Size([36600, 512])
computing unormed situation
tensor(0.3890, device='cuda:1') tensor(0.2774, device='cuda:1')
computing normed situation
tensor(0.4484, device='cuda:1') tensor(0.3608, device='cuda:1')
un-norm situation:
learned: map: 0.3890175223350525, prec: 0.2773630619049072
norm situation:
learned: map: 0.44837671518325806, prec: 0.3607600927352905
udcdr == 1

Query Emb Dim:torch.Size([1883, 512]); Gallery Emb Dim:torch.Size([5857, 512])
computing unormed situation
tensor(0.5191, device='cuda:1') tensor(0.2879, device='cuda:1')
computing normed situation
tensor(0.5956, device='cuda:1') tensor(0.3653, device='cuda:1')
un-norm situation:
learned: map: 0.5191384553909302, prec: 0.28792884945869446
norm situation:
learned: map: 0.5956119894981384, prec: 0.36527085304260254
Epoch Time:3m42s lr:0.0005000 mAP:0.4484 prec:0.3608


***Training and Validation complete***
================Parameters Settings=================
Parameters:	Namespace(num_shots=16, position='all', params=['q', 'k', 'v'], r=6, alpha=1, dropout_rate=0.25, log_name='reviewer_1', encoder='both', visual='VPT', vptNumTokens=4, ctx_init='a photo of a', text='CoOp', textNumTokens=4, optimizer='adam', l2_reg=0.0, epochs=1, lr=0.0005, momentum=0.9, resume_dict=None, code_path='/home/user/Code/DePro', dataset_path='/data/UCDR/data', dataset='DomainNet', is_eccv_split=1, clip_backbone='ViT-B/32', CLS_NUM_TOKENS=300, DOM_NUM_TOKENS=5, debug_mode=0, dropout=0.5, seen_domain='quickdraw', holdout_domain='sketch', gallery_domain='real', include_auxillary_domains=1, udcdr=0, image_size=224, seed=0, batch_size=50, num_workers=6, early_stop=2, log_interval=100)
================Training Settings=================
lr = 0.0005
batch_size = 50
==================================================
Residual Attention Block 0: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 1: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 2: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 3: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 4: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 5: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 6: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 7: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 8: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 9: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 10: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 11: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 0: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 1: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 2: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 3: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 4: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 5: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 6: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 7: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 8: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 9: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 10: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 11: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
======== Context-aware Simulator Learning Setup========
visual.transformer.resblocks.0.attn.q_proj.w_lora_A
visual.transformer.resblocks.0.attn.q_proj.w_lora_B
visual.transformer.resblocks.0.attn.k_proj.w_lora_A
visual.transformer.resblocks.0.attn.k_proj.w_lora_B
visual.transformer.resblocks.0.attn.v_proj.w_lora_A
visual.transformer.resblocks.0.attn.v_proj.w_lora_B
visual.transformer.resblocks.1.attn.q_proj.w_lora_A
visual.transformer.resblocks.1.attn.q_proj.w_lora_B
visual.transformer.resblocks.1.attn.k_proj.w_lora_A
visual.transformer.resblocks.1.attn.k_proj.w_lora_B
visual.transformer.resblocks.1.attn.v_proj.w_lora_A
visual.transformer.resblocks.1.attn.v_proj.w_lora_B
visual.transformer.resblocks.2.attn.q_proj.w_lora_A
visual.transformer.resblocks.2.attn.q_proj.w_lora_B
visual.transformer.resblocks.2.attn.k_proj.w_lora_A
visual.transformer.resblocks.2.attn.k_proj.w_lora_B
visual.transformer.resblocks.2.attn.v_proj.w_lora_A
visual.transformer.resblocks.2.attn.v_proj.w_lora_B
visual.transformer.resblocks.3.attn.q_proj.w_lora_A
visual.transformer.resblocks.3.attn.q_proj.w_lora_B
visual.transformer.resblocks.3.attn.k_proj.w_lora_A
visual.transformer.resblocks.3.attn.k_proj.w_lora_B
visual.transformer.resblocks.3.attn.v_proj.w_lora_A
visual.transformer.resblocks.3.attn.v_proj.w_lora_B
visual.transformer.resblocks.4.attn.q_proj.w_lora_A
visual.transformer.resblocks.4.attn.q_proj.w_lora_B
visual.transformer.resblocks.4.attn.k_proj.w_lora_A
visual.transformer.resblocks.4.attn.k_proj.w_lora_B
visual.transformer.resblocks.4.attn.v_proj.w_lora_A
visual.transformer.resblocks.4.attn.v_proj.w_lora_B
visual.transformer.resblocks.5.attn.q_proj.w_lora_A
visual.transformer.resblocks.5.attn.q_proj.w_lora_B
visual.transformer.resblocks.5.attn.k_proj.w_lora_A
visual.transformer.resblocks.5.attn.k_proj.w_lora_B
visual.transformer.resblocks.5.attn.v_proj.w_lora_A
visual.transformer.resblocks.5.attn.v_proj.w_lora_B
visual.transformer.resblocks.6.attn.q_proj.w_lora_A
visual.transformer.resblocks.6.attn.q_proj.w_lora_B
visual.transformer.resblocks.6.attn.k_proj.w_lora_A
visual.transformer.resblocks.6.attn.k_proj.w_lora_B
visual.transformer.resblocks.6.attn.v_proj.w_lora_A
visual.transformer.resblocks.6.attn.v_proj.w_lora_B
visual.transformer.resblocks.7.attn.q_proj.w_lora_A
visual.transformer.resblocks.7.attn.q_proj.w_lora_B
visual.transformer.resblocks.7.attn.k_proj.w_lora_A
visual.transformer.resblocks.7.attn.k_proj.w_lora_B
visual.transformer.resblocks.7.attn.v_proj.w_lora_A
visual.transformer.resblocks.7.attn.v_proj.w_lora_B
visual.transformer.resblocks.8.attn.q_proj.w_lora_A
visual.transformer.resblocks.8.attn.q_proj.w_lora_B
visual.transformer.resblocks.8.attn.k_proj.w_lora_A
visual.transformer.resblocks.8.attn.k_proj.w_lora_B
visual.transformer.resblocks.8.attn.v_proj.w_lora_A
visual.transformer.resblocks.8.attn.v_proj.w_lora_B
visual.transformer.resblocks.9.attn.q_proj.w_lora_A
visual.transformer.resblocks.9.attn.q_proj.w_lora_B
visual.transformer.resblocks.9.attn.k_proj.w_lora_A
visual.transformer.resblocks.9.attn.k_proj.w_lora_B
visual.transformer.resblocks.9.attn.v_proj.w_lora_A
visual.transformer.resblocks.9.attn.v_proj.w_lora_B
visual.transformer.resblocks.10.attn.q_proj.w_lora_A
visual.transformer.resblocks.10.attn.q_proj.w_lora_B
visual.transformer.resblocks.10.attn.k_proj.w_lora_A
visual.transformer.resblocks.10.attn.k_proj.w_lora_B
visual.transformer.resblocks.10.attn.v_proj.w_lora_A
visual.transformer.resblocks.10.attn.v_proj.w_lora_B
visual.transformer.resblocks.11.attn.q_proj.w_lora_A
visual.transformer.resblocks.11.attn.q_proj.w_lora_B
visual.transformer.resblocks.11.attn.k_proj.w_lora_A
visual.transformer.resblocks.11.attn.k_proj.w_lora_B
visual.transformer.resblocks.11.attn.v_proj.w_lora_A
visual.transformer.resblocks.11.attn.v_proj.w_lora_B
transformer.resblocks.0.attn.q_proj.w_lora_A
transformer.resblocks.0.attn.q_proj.w_lora_B
transformer.resblocks.0.attn.k_proj.w_lora_A
transformer.resblocks.0.attn.k_proj.w_lora_B
transformer.resblocks.0.attn.v_proj.w_lora_A
transformer.resblocks.0.attn.v_proj.w_lora_B
transformer.resblocks.1.attn.q_proj.w_lora_A
transformer.resblocks.1.attn.q_proj.w_lora_B
transformer.resblocks.1.attn.k_proj.w_lora_A
transformer.resblocks.1.attn.k_proj.w_lora_B
transformer.resblocks.1.attn.v_proj.w_lora_A
transformer.resblocks.1.attn.v_proj.w_lora_B
transformer.resblocks.2.attn.q_proj.w_lora_A
transformer.resblocks.2.attn.q_proj.w_lora_B
transformer.resblocks.2.attn.k_proj.w_lora_A
transformer.resblocks.2.attn.k_proj.w_lora_B
transformer.resblocks.2.attn.v_proj.w_lora_A
transformer.resblocks.2.attn.v_proj.w_lora_B
transformer.resblocks.3.attn.q_proj.w_lora_A
transformer.resblocks.3.attn.q_proj.w_lora_B
transformer.resblocks.3.attn.k_proj.w_lora_A
transformer.resblocks.3.attn.k_proj.w_lora_B
transformer.resblocks.3.attn.v_proj.w_lora_A
transformer.resblocks.3.attn.v_proj.w_lora_B
transformer.resblocks.4.attn.q_proj.w_lora_A
transformer.resblocks.4.attn.q_proj.w_lora_B
transformer.resblocks.4.attn.k_proj.w_lora_A
transformer.resblocks.4.attn.k_proj.w_lora_B
transformer.resblocks.4.attn.v_proj.w_lora_A
transformer.resblocks.4.attn.v_proj.w_lora_B
transformer.resblocks.5.attn.q_proj.w_lora_A
transformer.resblocks.5.attn.q_proj.w_lora_B
transformer.resblocks.5.attn.k_proj.w_lora_A
transformer.resblocks.5.attn.k_proj.w_lora_B
transformer.resblocks.5.attn.v_proj.w_lora_A
transformer.resblocks.5.attn.v_proj.w_lora_B
transformer.resblocks.6.attn.q_proj.w_lora_A
transformer.resblocks.6.attn.q_proj.w_lora_B
transformer.resblocks.6.attn.k_proj.w_lora_A
transformer.resblocks.6.attn.k_proj.w_lora_B
transformer.resblocks.6.attn.v_proj.w_lora_A
transformer.resblocks.6.attn.v_proj.w_lora_B
transformer.resblocks.7.attn.q_proj.w_lora_A
transformer.resblocks.7.attn.q_proj.w_lora_B
transformer.resblocks.7.attn.k_proj.w_lora_A
transformer.resblocks.7.attn.k_proj.w_lora_B
transformer.resblocks.7.attn.v_proj.w_lora_A
transformer.resblocks.7.attn.v_proj.w_lora_B
transformer.resblocks.8.attn.q_proj.w_lora_A
transformer.resblocks.8.attn.q_proj.w_lora_B
transformer.resblocks.8.attn.k_proj.w_lora_A
transformer.resblocks.8.attn.k_proj.w_lora_B
transformer.resblocks.8.attn.v_proj.w_lora_A
transformer.resblocks.8.attn.v_proj.w_lora_B
transformer.resblocks.9.attn.q_proj.w_lora_A
transformer.resblocks.9.attn.q_proj.w_lora_B
transformer.resblocks.9.attn.k_proj.w_lora_A
transformer.resblocks.9.attn.k_proj.w_lora_B
transformer.resblocks.9.attn.v_proj.w_lora_A
transformer.resblocks.9.attn.v_proj.w_lora_B
transformer.resblocks.10.attn.q_proj.w_lora_A
transformer.resblocks.10.attn.q_proj.w_lora_B
transformer.resblocks.10.attn.k_proj.w_lora_A
transformer.resblocks.10.attn.k_proj.w_lora_B
transformer.resblocks.10.attn.v_proj.w_lora_A
transformer.resblocks.10.attn.v_proj.w_lora_B
transformer.resblocks.11.attn.q_proj.w_lora_A
transformer.resblocks.11.attn.q_proj.w_lora_B
transformer.resblocks.11.attn.k_proj.w_lora_A
transformer.resblocks.11.attn.k_proj.w_lora_B
transformer.resblocks.11.attn.v_proj.w_lora_A
transformer.resblocks.11.attn.v_proj.w_lora_B
tot=151277313, train = 552960
===============================================
[Train] Epoch: [1/1][100/473]  Time 0.165 (0.315)  cls 3.2264 (3.0211)  contrastive 1.6194 (5.5288)  loss 4.8458 (8.5499)  
[Train] Epoch: [1/1][200/473]  Time 0.164 (0.320)  cls 1.9134 (2.7904)  contrastive 0.8064 (3.2361)  loss 2.7198 (6.0265)  
[Train] Epoch: [1/1][300/473]  Time 0.686 (0.319)  cls 2.8254 (2.7024)  contrastive 0.7083 (2.4425)  loss 3.5337 (5.1448)  
[Train] Epoch: [1/1][400/473]  Time 0.165 (0.321)  cls 2.6291 (2.6395)  contrastive 0.4715 (2.0357)  loss 3.1006 (4.6751)  
epoch = [1/1]loss = {'net': 4.45064539720766, 'acc': 0.48976311336717426}

***Validation***
udcdr == 0
Query:sketch; Gallery:real; Generalized:0

Query Emb Dim:torch.Size([9729, 512]); Gallery Emb Dim:torch.Size([24387, 512])
computing unormed situation
tensor(0.4567, device='cuda:1') tensor(0.3520, device='cuda:1')
computing normed situation
tensor(0.5148, device='cuda:1') tensor(0.4296, device='cuda:1')
un-norm situation:
learned: map: 0.45667794346809387, prec: 0.35203978419303894
norm situation:
learned: map: 0.5148372650146484, prec: 0.42957499623298645
Query:sketch; Gallery:real; Generalized:1

Query Emb Dim:torch.Size([9729, 512]); Gallery Emb Dim:torch.Size([36600, 512])
computing unormed situation
tensor(0.4142, device='cuda:1') tensor(0.3091, device='cuda:1')
computing normed situation
tensor(0.4639, device='cuda:1') tensor(0.3804, device='cuda:1')
un-norm situation:
learned: map: 0.4142243266105652, prec: 0.30907389521598816
norm situation:
learned: map: 0.4638781249523163, prec: 0.3803849518299103
udcdr == 1

Query Emb Dim:torch.Size([1883, 512]); Gallery Emb Dim:torch.Size([5857, 512])
computing unormed situation
tensor(0.5507, device='cuda:1') tensor(0.3227, device='cuda:1')
computing normed situation
tensor(0.6148, device='cuda:1') tensor(0.3871, device='cuda:1')
un-norm situation:
learned: map: 0.5506541728973389, prec: 0.3226712644100189
norm situation:
learned: map: 0.6148493885993958, prec: 0.38709506392478943
Epoch Time:4m52s lr:0.0005000 mAP:0.4639 prec:0.3804


***Training and Validation complete***
