================Parameters Settings=================
Parameters:	Namespace(num_shots=1, position='all', params=['q', 'k', 'v'], r=6, alpha=1, dropout_rate=0.25, log_name='reviewer_1', encoder='both', visual='VPT', vptNumTokens=4, ctx_init='a photo of a', text='CoOp', textNumTokens=4, optimizer='adam', l2_reg=0.0, epochs=1, lr=0.0005, momentum=0.9, resume_dict=None, code_path='/home/user/Code/DePro', dataset_path='/data/UCDR/data', dataset='DomainNet', is_eccv_split=1, clip_backbone='ViT-B/32', CLS_NUM_TOKENS=300, DOM_NUM_TOKENS=5, debug_mode=0, dropout=0.5, seen_domain='painting', holdout_domain='clipart', gallery_domain='real', include_auxillary_domains=1, udcdr=0, image_size=224, seed=0, batch_size=50, num_workers=6, early_stop=2, log_interval=100)
================Training Settings=================
lr = 0.0005
batch_size = 50
==================================================
Residual Attention Block 0: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 1: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 2: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 3: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 4: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 5: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 6: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 7: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 8: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 9: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 10: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 11: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 0: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 1: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 2: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 3: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 4: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 5: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 6: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 7: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 8: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 9: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 10: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 11: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
======== Context-aware Simulator Learning Setup========
visual.transformer.resblocks.0.attn.q_proj.w_lora_A
visual.transformer.resblocks.0.attn.q_proj.w_lora_B
visual.transformer.resblocks.0.attn.k_proj.w_lora_A
visual.transformer.resblocks.0.attn.k_proj.w_lora_B
visual.transformer.resblocks.0.attn.v_proj.w_lora_A
visual.transformer.resblocks.0.attn.v_proj.w_lora_B
visual.transformer.resblocks.1.attn.q_proj.w_lora_A
visual.transformer.resblocks.1.attn.q_proj.w_lora_B
visual.transformer.resblocks.1.attn.k_proj.w_lora_A
visual.transformer.resblocks.1.attn.k_proj.w_lora_B
visual.transformer.resblocks.1.attn.v_proj.w_lora_A
visual.transformer.resblocks.1.attn.v_proj.w_lora_B
visual.transformer.resblocks.2.attn.q_proj.w_lora_A
visual.transformer.resblocks.2.attn.q_proj.w_lora_B
visual.transformer.resblocks.2.attn.k_proj.w_lora_A
visual.transformer.resblocks.2.attn.k_proj.w_lora_B
visual.transformer.resblocks.2.attn.v_proj.w_lora_A
visual.transformer.resblocks.2.attn.v_proj.w_lora_B
visual.transformer.resblocks.3.attn.q_proj.w_lora_A
visual.transformer.resblocks.3.attn.q_proj.w_lora_B
visual.transformer.resblocks.3.attn.k_proj.w_lora_A
visual.transformer.resblocks.3.attn.k_proj.w_lora_B
visual.transformer.resblocks.3.attn.v_proj.w_lora_A
visual.transformer.resblocks.3.attn.v_proj.w_lora_B
visual.transformer.resblocks.4.attn.q_proj.w_lora_A
visual.transformer.resblocks.4.attn.q_proj.w_lora_B
visual.transformer.resblocks.4.attn.k_proj.w_lora_A
visual.transformer.resblocks.4.attn.k_proj.w_lora_B
visual.transformer.resblocks.4.attn.v_proj.w_lora_A
visual.transformer.resblocks.4.attn.v_proj.w_lora_B
visual.transformer.resblocks.5.attn.q_proj.w_lora_A
visual.transformer.resblocks.5.attn.q_proj.w_lora_B
visual.transformer.resblocks.5.attn.k_proj.w_lora_A
visual.transformer.resblocks.5.attn.k_proj.w_lora_B
visual.transformer.resblocks.5.attn.v_proj.w_lora_A
visual.transformer.resblocks.5.attn.v_proj.w_lora_B
visual.transformer.resblocks.6.attn.q_proj.w_lora_A
visual.transformer.resblocks.6.attn.q_proj.w_lora_B
visual.transformer.resblocks.6.attn.k_proj.w_lora_A
visual.transformer.resblocks.6.attn.k_proj.w_lora_B
visual.transformer.resblocks.6.attn.v_proj.w_lora_A
visual.transformer.resblocks.6.attn.v_proj.w_lora_B
visual.transformer.resblocks.7.attn.q_proj.w_lora_A
visual.transformer.resblocks.7.attn.q_proj.w_lora_B
visual.transformer.resblocks.7.attn.k_proj.w_lora_A
visual.transformer.resblocks.7.attn.k_proj.w_lora_B
visual.transformer.resblocks.7.attn.v_proj.w_lora_A
visual.transformer.resblocks.7.attn.v_proj.w_lora_B
visual.transformer.resblocks.8.attn.q_proj.w_lora_A
visual.transformer.resblocks.8.attn.q_proj.w_lora_B
visual.transformer.resblocks.8.attn.k_proj.w_lora_A
visual.transformer.resblocks.8.attn.k_proj.w_lora_B
visual.transformer.resblocks.8.attn.v_proj.w_lora_A
visual.transformer.resblocks.8.attn.v_proj.w_lora_B
visual.transformer.resblocks.9.attn.q_proj.w_lora_A
visual.transformer.resblocks.9.attn.q_proj.w_lora_B
visual.transformer.resblocks.9.attn.k_proj.w_lora_A
visual.transformer.resblocks.9.attn.k_proj.w_lora_B
visual.transformer.resblocks.9.attn.v_proj.w_lora_A
visual.transformer.resblocks.9.attn.v_proj.w_lora_B
visual.transformer.resblocks.10.attn.q_proj.w_lora_A
visual.transformer.resblocks.10.attn.q_proj.w_lora_B
visual.transformer.resblocks.10.attn.k_proj.w_lora_A
visual.transformer.resblocks.10.attn.k_proj.w_lora_B
visual.transformer.resblocks.10.attn.v_proj.w_lora_A
visual.transformer.resblocks.10.attn.v_proj.w_lora_B
visual.transformer.resblocks.11.attn.q_proj.w_lora_A
visual.transformer.resblocks.11.attn.q_proj.w_lora_B
visual.transformer.resblocks.11.attn.k_proj.w_lora_A
visual.transformer.resblocks.11.attn.k_proj.w_lora_B
visual.transformer.resblocks.11.attn.v_proj.w_lora_A
visual.transformer.resblocks.11.attn.v_proj.w_lora_B
transformer.resblocks.0.attn.q_proj.w_lora_A
transformer.resblocks.0.attn.q_proj.w_lora_B
transformer.resblocks.0.attn.k_proj.w_lora_A
transformer.resblocks.0.attn.k_proj.w_lora_B
transformer.resblocks.0.attn.v_proj.w_lora_A
transformer.resblocks.0.attn.v_proj.w_lora_B
transformer.resblocks.1.attn.q_proj.w_lora_A
transformer.resblocks.1.attn.q_proj.w_lora_B
transformer.resblocks.1.attn.k_proj.w_lora_A
transformer.resblocks.1.attn.k_proj.w_lora_B
transformer.resblocks.1.attn.v_proj.w_lora_A
transformer.resblocks.1.attn.v_proj.w_lora_B
transformer.resblocks.2.attn.q_proj.w_lora_A
transformer.resblocks.2.attn.q_proj.w_lora_B
transformer.resblocks.2.attn.k_proj.w_lora_A
transformer.resblocks.2.attn.k_proj.w_lora_B
transformer.resblocks.2.attn.v_proj.w_lora_A
transformer.resblocks.2.attn.v_proj.w_lora_B
transformer.resblocks.3.attn.q_proj.w_lora_A
transformer.resblocks.3.attn.q_proj.w_lora_B
transformer.resblocks.3.attn.k_proj.w_lora_A
transformer.resblocks.3.attn.k_proj.w_lora_B
transformer.resblocks.3.attn.v_proj.w_lora_A
transformer.resblocks.3.attn.v_proj.w_lora_B
transformer.resblocks.4.attn.q_proj.w_lora_A
transformer.resblocks.4.attn.q_proj.w_lora_B
transformer.resblocks.4.attn.k_proj.w_lora_A
transformer.resblocks.4.attn.k_proj.w_lora_B
transformer.resblocks.4.attn.v_proj.w_lora_A
transformer.resblocks.4.attn.v_proj.w_lora_B
transformer.resblocks.5.attn.q_proj.w_lora_A
transformer.resblocks.5.attn.q_proj.w_lora_B
transformer.resblocks.5.attn.k_proj.w_lora_A
transformer.resblocks.5.attn.k_proj.w_lora_B
transformer.resblocks.5.attn.v_proj.w_lora_A
transformer.resblocks.5.attn.v_proj.w_lora_B
transformer.resblocks.6.attn.q_proj.w_lora_A
transformer.resblocks.6.attn.q_proj.w_lora_B
transformer.resblocks.6.attn.k_proj.w_lora_A
transformer.resblocks.6.attn.k_proj.w_lora_B
transformer.resblocks.6.attn.v_proj.w_lora_A
transformer.resblocks.6.attn.v_proj.w_lora_B
transformer.resblocks.7.attn.q_proj.w_lora_A
transformer.resblocks.7.attn.q_proj.w_lora_B
transformer.resblocks.7.attn.k_proj.w_lora_A
transformer.resblocks.7.attn.k_proj.w_lora_B
transformer.resblocks.7.attn.v_proj.w_lora_A
transformer.resblocks.7.attn.v_proj.w_lora_B
transformer.resblocks.8.attn.q_proj.w_lora_A
transformer.resblocks.8.attn.q_proj.w_lora_B
transformer.resblocks.8.attn.k_proj.w_lora_A
transformer.resblocks.8.attn.k_proj.w_lora_B
transformer.resblocks.8.attn.v_proj.w_lora_A
transformer.resblocks.8.attn.v_proj.w_lora_B
transformer.resblocks.9.attn.q_proj.w_lora_A
transformer.resblocks.9.attn.q_proj.w_lora_B
transformer.resblocks.9.attn.k_proj.w_lora_A
transformer.resblocks.9.attn.k_proj.w_lora_B
transformer.resblocks.9.attn.v_proj.w_lora_A
transformer.resblocks.9.attn.v_proj.w_lora_B
transformer.resblocks.10.attn.q_proj.w_lora_A
transformer.resblocks.10.attn.q_proj.w_lora_B
transformer.resblocks.10.attn.k_proj.w_lora_A
transformer.resblocks.10.attn.k_proj.w_lora_B
transformer.resblocks.10.attn.v_proj.w_lora_A
transformer.resblocks.10.attn.v_proj.w_lora_B
transformer.resblocks.11.attn.q_proj.w_lora_A
transformer.resblocks.11.attn.q_proj.w_lora_B
transformer.resblocks.11.attn.k_proj.w_lora_A
transformer.resblocks.11.attn.k_proj.w_lora_B
transformer.resblocks.11.attn.v_proj.w_lora_A
transformer.resblocks.11.attn.v_proj.w_lora_B
tot=151277313, train = 552960
===============================================
epoch = [1/1]loss = {'net': 5.361757651964823, 'acc': 0.4493333333333333}

***Validation***
udcdr == 0
Query:clipart; Gallery:real; Generalized:0

Query Emb Dim:torch.Size([6394, 512]); Gallery Emb Dim:torch.Size([24387, 512])
computing unormed situation
tensor(0.6565, device='cuda:1') tensor(0.5757, device='cuda:1')
computing normed situation
tensor(0.6544, device='cuda:1') tensor(0.5636, device='cuda:1')
un-norm situation:
learned: map: 0.6565372943878174, prec: 0.5756560564041138
norm situation:
learned: map: 0.6543921828269958, prec: 0.5635705590248108
Query:clipart; Gallery:real; Generalized:1

Query Emb Dim:torch.Size([6394, 512]); Gallery Emb Dim:torch.Size([36600, 512])
computing unormed situation
tensor(0.6121, device='cuda:1') tensor(0.5311, device='cuda:1')
computing normed situation
tensor(0.6112, device='cuda:1') tensor(0.5231, device='cuda:1')
un-norm situation:
learned: map: 0.6120567321777344, prec: 0.5310799479484558
norm situation:
learned: map: 0.6112463474273682, prec: 0.5231490731239319
udcdr == 1

Query Emb Dim:torch.Size([1180, 512]); Gallery Emb Dim:torch.Size([5857, 512])
computing unormed situation
tensor(0.6107, device='cuda:1') tensor(0.3535, device='cuda:1')
computing normed situation
tensor(0.5964, device='cuda:1') tensor(0.3238, device='cuda:1')
un-norm situation:
learned: map: 0.6107405424118042, prec: 0.35349151492118835
norm situation:
learned: map: 0.5964162945747375, prec: 0.32378390431404114
Epoch Time:1m55s lr:0.0005000 mAP:0.6112 prec:0.5231


***Training and Validation complete***
================Parameters Settings=================
Parameters:	Namespace(num_shots=2, position='all', params=['q', 'k', 'v'], r=6, alpha=1, dropout_rate=0.25, log_name='reviewer_1', encoder='both', visual='VPT', vptNumTokens=4, ctx_init='a photo of a', text='CoOp', textNumTokens=4, optimizer='adam', l2_reg=0.0, epochs=1, lr=0.0005, momentum=0.9, resume_dict=None, code_path='/home/user/Code/DePro', dataset_path='/data/UCDR/data', dataset='DomainNet', is_eccv_split=1, clip_backbone='ViT-B/32', CLS_NUM_TOKENS=300, DOM_NUM_TOKENS=5, debug_mode=0, dropout=0.5, seen_domain='painting', holdout_domain='clipart', gallery_domain='real', include_auxillary_domains=1, udcdr=0, image_size=224, seed=0, batch_size=50, num_workers=6, early_stop=2, log_interval=100)
================Training Settings=================
lr = 0.0005
batch_size = 50
==================================================
Residual Attention Block 0: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 1: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 2: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 3: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 4: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 5: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 6: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 7: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 8: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 9: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 10: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 11: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 0: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 1: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 2: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 3: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 4: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 5: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 6: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 7: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 8: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 9: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 10: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 11: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)


  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
======== Context-aware Simulator Learning Setup========
visual.transformer.resblocks.0.attn.q_proj.w_lora_A
visual.transformer.resblocks.0.attn.q_proj.w_lora_B
visual.transformer.resblocks.0.attn.k_proj.w_lora_A
visual.transformer.resblocks.0.attn.k_proj.w_lora_B
visual.transformer.resblocks.0.attn.v_proj.w_lora_A
visual.transformer.resblocks.0.attn.v_proj.w_lora_B
visual.transformer.resblocks.1.attn.q_proj.w_lora_A
visual.transformer.resblocks.1.attn.q_proj.w_lora_B
visual.transformer.resblocks.1.attn.k_proj.w_lora_A
visual.transformer.resblocks.1.attn.k_proj.w_lora_B
visual.transformer.resblocks.1.attn.v_proj.w_lora_A
visual.transformer.resblocks.1.attn.v_proj.w_lora_B
visual.transformer.resblocks.2.attn.q_proj.w_lora_A
visual.transformer.resblocks.2.attn.q_proj.w_lora_B
visual.transformer.resblocks.2.attn.k_proj.w_lora_A
visual.transformer.resblocks.2.attn.k_proj.w_lora_B
visual.transformer.resblocks.2.attn.v_proj.w_lora_A
visual.transformer.resblocks.2.attn.v_proj.w_lora_B
visual.transformer.resblocks.3.attn.q_proj.w_lora_A
visual.transformer.resblocks.3.attn.q_proj.w_lora_B
visual.transformer.resblocks.3.attn.k_proj.w_lora_A
visual.transformer.resblocks.3.attn.k_proj.w_lora_B
visual.transformer.resblocks.3.attn.v_proj.w_lora_A
visual.transformer.resblocks.3.attn.v_proj.w_lora_B
visual.transformer.resblocks.4.attn.q_proj.w_lora_A
visual.transformer.resblocks.4.attn.q_proj.w_lora_B
visual.transformer.resblocks.4.attn.k_proj.w_lora_A
visual.transformer.resblocks.4.attn.k_proj.w_lora_B
visual.transformer.resblocks.4.attn.v_proj.w_lora_A
visual.transformer.resblocks.4.attn.v_proj.w_lora_B
visual.transformer.resblocks.5.attn.q_proj.w_lora_A
visual.transformer.resblocks.5.attn.q_proj.w_lora_B
visual.transformer.resblocks.5.attn.k_proj.w_lora_A
visual.transformer.resblocks.5.attn.k_proj.w_lora_B
visual.transformer.resblocks.5.attn.v_proj.w_lora_A
visual.transformer.resblocks.5.attn.v_proj.w_lora_B
visual.transformer.resblocks.6.attn.q_proj.w_lora_A
visual.transformer.resblocks.6.attn.q_proj.w_lora_B
visual.transformer.resblocks.6.attn.k_proj.w_lora_A
visual.transformer.resblocks.6.attn.k_proj.w_lora_B
visual.transformer.resblocks.6.attn.v_proj.w_lora_A
visual.transformer.resblocks.6.attn.v_proj.w_lora_B
visual.transformer.resblocks.7.attn.q_proj.w_lora_A
visual.transformer.resblocks.7.attn.q_proj.w_lora_B
visual.transformer.resblocks.7.attn.k_proj.w_lora_A
visual.transformer.resblocks.7.attn.k_proj.w_lora_B
visual.transformer.resblocks.7.attn.v_proj.w_lora_A
visual.transformer.resblocks.7.attn.v_proj.w_lora_B
visual.transformer.resblocks.8.attn.q_proj.w_lora_A
visual.transformer.resblocks.8.attn.q_proj.w_lora_B
visual.transformer.resblocks.8.attn.k_proj.w_lora_A
visual.transformer.resblocks.8.attn.k_proj.w_lora_B
visual.transformer.resblocks.8.attn.v_proj.w_lora_A
visual.transformer.resblocks.8.attn.v_proj.w_lora_B
visual.transformer.resblocks.9.attn.q_proj.w_lora_A
visual.transformer.resblocks.9.attn.q_proj.w_lora_B
visual.transformer.resblocks.9.attn.k_proj.w_lora_A
visual.transformer.resblocks.9.attn.k_proj.w_lora_B
visual.transformer.resblocks.9.attn.v_proj.w_lora_A
visual.transformer.resblocks.9.attn.v_proj.w_lora_B
visual.transformer.resblocks.10.attn.q_proj.w_lora_A
visual.transformer.resblocks.10.attn.q_proj.w_lora_B
visual.transformer.resblocks.10.attn.k_proj.w_lora_A
visual.transformer.resblocks.10.attn.k_proj.w_lora_B
visual.transformer.resblocks.10.attn.v_proj.w_lora_A
visual.transformer.resblocks.10.attn.v_proj.w_lora_B
visual.transformer.resblocks.11.attn.q_proj.w_lora_A
visual.transformer.resblocks.11.attn.q_proj.w_lora_B
visual.transformer.resblocks.11.attn.k_proj.w_lora_A
visual.transformer.resblocks.11.attn.k_proj.w_lora_B
visual.transformer.resblocks.11.attn.v_proj.w_lora_A
visual.transformer.resblocks.11.attn.v_proj.w_lora_B
transformer.resblocks.0.attn.q_proj.w_lora_A
transformer.resblocks.0.attn.q_proj.w_lora_B
transformer.resblocks.0.attn.k_proj.w_lora_A
transformer.resblocks.0.attn.k_proj.w_lora_B
transformer.resblocks.0.attn.v_proj.w_lora_A
transformer.resblocks.0.attn.v_proj.w_lora_B
transformer.resblocks.1.attn.q_proj.w_lora_A
transformer.resblocks.1.attn.q_proj.w_lora_B
transformer.resblocks.1.attn.k_proj.w_lora_A
transformer.resblocks.1.attn.k_proj.w_lora_B
transformer.resblocks.1.attn.v_proj.w_lora_A
transformer.resblocks.1.attn.v_proj.w_lora_B
transformer.resblocks.2.attn.q_proj.w_lora_A
transformer.resblocks.2.attn.q_proj.w_lora_B
transformer.resblocks.2.attn.k_proj.w_lora_A
transformer.resblocks.2.attn.k_proj.w_lora_B
transformer.resblocks.2.attn.v_proj.w_lora_A
transformer.resblocks.2.attn.v_proj.w_lora_B
transformer.resblocks.3.attn.q_proj.w_lora_A
transformer.resblocks.3.attn.q_proj.w_lora_B
transformer.resblocks.3.attn.k_proj.w_lora_A
transformer.resblocks.3.attn.k_proj.w_lora_B
transformer.resblocks.3.attn.v_proj.w_lora_A
transformer.resblocks.3.attn.v_proj.w_lora_B
transformer.resblocks.4.attn.q_proj.w_lora_A
transformer.resblocks.4.attn.q_proj.w_lora_B
transformer.resblocks.4.attn.k_proj.w_lora_A
transformer.resblocks.4.attn.k_proj.w_lora_B
transformer.resblocks.4.attn.v_proj.w_lora_A
transformer.resblocks.4.attn.v_proj.w_lora_B
transformer.resblocks.5.attn.q_proj.w_lora_A
transformer.resblocks.5.attn.q_proj.w_lora_B
transformer.resblocks.5.attn.k_proj.w_lora_A
transformer.resblocks.5.attn.k_proj.w_lora_B
transformer.resblocks.5.attn.v_proj.w_lora_A
transformer.resblocks.5.attn.v_proj.w_lora_B
transformer.resblocks.6.attn.q_proj.w_lora_A
transformer.resblocks.6.attn.q_proj.w_lora_B
transformer.resblocks.6.attn.k_proj.w_lora_A
transformer.resblocks.6.attn.k_proj.w_lora_B
transformer.resblocks.6.attn.v_proj.w_lora_A
transformer.resblocks.6.attn.v_proj.w_lora_B
transformer.resblocks.7.attn.q_proj.w_lora_A
transformer.resblocks.7.attn.q_proj.w_lora_B
transformer.resblocks.7.attn.k_proj.w_lora_A
transformer.resblocks.7.attn.k_proj.w_lora_B
transformer.resblocks.7.attn.v_proj.w_lora_A
transformer.resblocks.7.attn.v_proj.w_lora_B
transformer.resblocks.8.attn.q_proj.w_lora_A
transformer.resblocks.8.attn.q_proj.w_lora_B
transformer.resblocks.8.attn.k_proj.w_lora_A
transformer.resblocks.8.attn.k_proj.w_lora_B
transformer.resblocks.8.attn.v_proj.w_lora_A
transformer.resblocks.8.attn.v_proj.w_lora_B
transformer.resblocks.9.attn.q_proj.w_lora_A
transformer.resblocks.9.attn.q_proj.w_lora_B
transformer.resblocks.9.attn.k_proj.w_lora_A
transformer.resblocks.9.attn.k_proj.w_lora_B
transformer.resblocks.9.attn.v_proj.w_lora_A
transformer.resblocks.9.attn.v_proj.w_lora_B
transformer.resblocks.10.attn.q_proj.w_lora_A
transformer.resblocks.10.attn.q_proj.w_lora_B
transformer.resblocks.10.attn.k_proj.w_lora_A
transformer.resblocks.10.attn.k_proj.w_lora_B
transformer.resblocks.10.attn.v_proj.w_lora_A
transformer.resblocks.10.attn.v_proj.w_lora_B
transformer.resblocks.11.attn.q_proj.w_lora_A
transformer.resblocks.11.attn.q_proj.w_lora_B
transformer.resblocks.11.attn.k_proj.w_lora_A
transformer.resblocks.11.attn.k_proj.w_lora_B
transformer.resblocks.11.attn.v_proj.w_lora_A
transformer.resblocks.11.attn.v_proj.w_lora_B
tot=151277313, train = 552960
===============================================
epoch = [1/1]loss = {'net': 9.772215243180593, 'acc': 0.44233333333333336}

***Validation***
udcdr == 0
Query:clipart; Gallery:real; Generalized:0

Query Emb Dim:torch.Size([6394, 512]); Gallery Emb Dim:torch.Size([24387, 512])
computing unormed situation
tensor(0.6247, device='cuda:1') tensor(0.5257, device='cuda:1')
computing normed situation
tensor(0.7052, device='cuda:1') tensor(0.6411, device='cuda:1')
un-norm situation:
learned: map: 0.6246508359909058, prec: 0.525749146938324
norm situation:
learned: map: 0.7051952481269836, prec: 0.641116738319397
Query:clipart; Gallery:real; Generalized:1

Query Emb Dim:torch.Size([6394, 512]); Gallery Emb Dim:torch.Size([36600, 512])
computing unormed situation
tensor(0.5848, device='cuda:1') tensor(0.4842, device='cuda:1')
computing normed situation
tensor(0.6600, device='cuda:1') tensor(0.5963, device='cuda:1')
un-norm situation:
learned: map: 0.5848401188850403, prec: 0.4842414855957031
norm situation:
learned: map: 0.6600438356399536, prec: 0.596287190914154
udcdr == 1

Query Emb Dim:torch.Size([1180, 512]); Gallery Emb Dim:torch.Size([5857, 512])
computing unormed situation
tensor(0.6038, device='cuda:1') tensor(0.3545, device='cuda:1')
computing normed situation
tensor(0.6956, device='cuda:1') tensor(0.4177, device='cuda:1')
un-norm situation:
learned: map: 0.6037737131118774, prec: 0.35452964901924133
norm situation:
learned: map: 0.6955661773681641, prec: 0.41767796874046326
Epoch Time:1m60s lr:0.0005000 mAP:0.6600 prec:0.5963


***Training and Validation complete***
================Parameters Settings=================
Parameters:	Namespace(num_shots=4, position='all', params=['q', 'k', 'v'], r=6, alpha=1, dropout_rate=0.25, log_name='reviewer_1', encoder='both', visual='VPT', vptNumTokens=4, ctx_init='a photo of a', text='CoOp', textNumTokens=4, optimizer='adam', l2_reg=0.0, epochs=1, lr=0.0005, momentum=0.9, resume_dict=None, code_path='/home/user/Code/DePro', dataset_path='/data/UCDR/data', dataset='DomainNet', is_eccv_split=1, clip_backbone='ViT-B/32', CLS_NUM_TOKENS=300, DOM_NUM_TOKENS=5, debug_mode=0, dropout=0.5, seen_domain='painting', holdout_domain='clipart', gallery_domain='real', include_auxillary_domains=1, udcdr=0, image_size=224, seed=0, batch_size=50, num_workers=6, early_stop=2, log_interval=100)
================Training Settings=================
lr = 0.0005
batch_size = 50
==================================================
Residual Attention Block 0: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 1: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 2: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 3: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 4: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 5: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 6: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 7: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 8: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 9: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 10: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 11: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 0: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 1: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 2: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 3: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 4: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 5: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 6: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 7: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 8: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 9: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 10: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 11: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
======== Context-aware Simulator Learning Setup========
visual.transformer.resblocks.0.attn.q_proj.w_lora_A
visual.transformer.resblocks.0.attn.q_proj.w_lora_B
visual.transformer.resblocks.0.attn.k_proj.w_lora_A
visual.transformer.resblocks.0.attn.k_proj.w_lora_B
visual.transformer.resblocks.0.attn.v_proj.w_lora_A
visual.transformer.resblocks.0.attn.v_proj.w_lora_B
visual.transformer.resblocks.1.attn.q_proj.w_lora_A
visual.transformer.resblocks.1.attn.q_proj.w_lora_B
visual.transformer.resblocks.1.attn.k_proj.w_lora_A
visual.transformer.resblocks.1.attn.k_proj.w_lora_B
visual.transformer.resblocks.1.attn.v_proj.w_lora_A
visual.transformer.resblocks.1.attn.v_proj.w_lora_B
visual.transformer.resblocks.2.attn.q_proj.w_lora_A
visual.transformer.resblocks.2.attn.q_proj.w_lora_B
visual.transformer.resblocks.2.attn.k_proj.w_lora_A
visual.transformer.resblocks.2.attn.k_proj.w_lora_B
visual.transformer.resblocks.2.attn.v_proj.w_lora_A
visual.transformer.resblocks.2.attn.v_proj.w_lora_B
visual.transformer.resblocks.3.attn.q_proj.w_lora_A
visual.transformer.resblocks.3.attn.q_proj.w_lora_B
visual.transformer.resblocks.3.attn.k_proj.w_lora_A
visual.transformer.resblocks.3.attn.k_proj.w_lora_B
visual.transformer.resblocks.3.attn.v_proj.w_lora_A
visual.transformer.resblocks.3.attn.v_proj.w_lora_B
visual.transformer.resblocks.4.attn.q_proj.w_lora_A
visual.transformer.resblocks.4.attn.q_proj.w_lora_B
visual.transformer.resblocks.4.attn.k_proj.w_lora_A
visual.transformer.resblocks.4.attn.k_proj.w_lora_B
visual.transformer.resblocks.4.attn.v_proj.w_lora_A
visual.transformer.resblocks.4.attn.v_proj.w_lora_B
visual.transformer.resblocks.5.attn.q_proj.w_lora_A
visual.transformer.resblocks.5.attn.q_proj.w_lora_B
visual.transformer.resblocks.5.attn.k_proj.w_lora_A
visual.transformer.resblocks.5.attn.k_proj.w_lora_B
visual.transformer.resblocks.5.attn.v_proj.w_lora_A
visual.transformer.resblocks.5.attn.v_proj.w_lora_B
visual.transformer.resblocks.6.attn.q_proj.w_lora_A
visual.transformer.resblocks.6.attn.q_proj.w_lora_B
visual.transformer.resblocks.6.attn.k_proj.w_lora_A
visual.transformer.resblocks.6.attn.k_proj.w_lora_B
visual.transformer.resblocks.6.attn.v_proj.w_lora_A
visual.transformer.resblocks.6.attn.v_proj.w_lora_B
visual.transformer.resblocks.7.attn.q_proj.w_lora_A
visual.transformer.resblocks.7.attn.q_proj.w_lora_B
visual.transformer.resblocks.7.attn.k_proj.w_lora_A
visual.transformer.resblocks.7.attn.k_proj.w_lora_B
visual.transformer.resblocks.7.attn.v_proj.w_lora_A
visual.transformer.resblocks.7.attn.v_proj.w_lora_B
visual.transformer.resblocks.8.attn.q_proj.w_lora_A
visual.transformer.resblocks.8.attn.q_proj.w_lora_B
visual.transformer.resblocks.8.attn.k_proj.w_lora_A
visual.transformer.resblocks.8.attn.k_proj.w_lora_B
visual.transformer.resblocks.8.attn.v_proj.w_lora_A
visual.transformer.resblocks.8.attn.v_proj.w_lora_B
visual.transformer.resblocks.9.attn.q_proj.w_lora_A
visual.transformer.resblocks.9.attn.q_proj.w_lora_B
visual.transformer.resblocks.9.attn.k_proj.w_lora_A
visual.transformer.resblocks.9.attn.k_proj.w_lora_B
visual.transformer.resblocks.9.attn.v_proj.w_lora_A
visual.transformer.resblocks.9.attn.v_proj.w_lora_B
visual.transformer.resblocks.10.attn.q_proj.w_lora_A
visual.transformer.resblocks.10.attn.q_proj.w_lora_B
visual.transformer.resblocks.10.attn.k_proj.w_lora_A
visual.transformer.resblocks.10.attn.k_proj.w_lora_B
visual.transformer.resblocks.10.attn.v_proj.w_lora_A
visual.transformer.resblocks.10.attn.v_proj.w_lora_B
visual.transformer.resblocks.11.attn.q_proj.w_lora_A
visual.transformer.resblocks.11.attn.q_proj.w_lora_B
visual.transformer.resblocks.11.attn.k_proj.w_lora_A
visual.transformer.resblocks.11.attn.k_proj.w_lora_B
visual.transformer.resblocks.11.attn.v_proj.w_lora_A
visual.transformer.resblocks.11.attn.v_proj.w_lora_B
transformer.resblocks.0.attn.q_proj.w_lora_A
transformer.resblocks.0.attn.q_proj.w_lora_B
transformer.resblocks.0.attn.k_proj.w_lora_A
transformer.resblocks.0.attn.k_proj.w_lora_B
transformer.resblocks.0.attn.v_proj.w_lora_A
transformer.resblocks.0.attn.v_proj.w_lora_B
transformer.resblocks.1.attn.q_proj.w_lora_A
transformer.resblocks.1.attn.q_proj.w_lora_B
transformer.resblocks.1.attn.k_proj.w_lora_A
transformer.resblocks.1.attn.k_proj.w_lora_B
transformer.resblocks.1.attn.v_proj.w_lora_A
transformer.resblocks.1.attn.v_proj.w_lora_B
transformer.resblocks.2.attn.q_proj.w_lora_A
transformer.resblocks.2.attn.q_proj.w_lora_B
transformer.resblocks.2.attn.k_proj.w_lora_A
transformer.resblocks.2.attn.k_proj.w_lora_B
transformer.resblocks.2.attn.v_proj.w_lora_A
transformer.resblocks.2.attn.v_proj.w_lora_B
transformer.resblocks.3.attn.q_proj.w_lora_A
transformer.resblocks.3.attn.q_proj.w_lora_B
transformer.resblocks.3.attn.k_proj.w_lora_A
transformer.resblocks.3.attn.k_proj.w_lora_B
transformer.resblocks.3.attn.v_proj.w_lora_A
transformer.resblocks.3.attn.v_proj.w_lora_B
transformer.resblocks.4.attn.q_proj.w_lora_A
transformer.resblocks.4.attn.q_proj.w_lora_B
transformer.resblocks.4.attn.k_proj.w_lora_A
transformer.resblocks.4.attn.k_proj.w_lora_B
transformer.resblocks.4.attn.v_proj.w_lora_A
transformer.resblocks.4.attn.v_proj.w_lora_B
transformer.resblocks.5.attn.q_proj.w_lora_A
transformer.resblocks.5.attn.q_proj.w_lora_B
transformer.resblocks.5.attn.k_proj.w_lora_A
transformer.resblocks.5.attn.k_proj.w_lora_B
transformer.resblocks.5.attn.v_proj.w_lora_A
transformer.resblocks.5.attn.v_proj.w_lora_B
transformer.resblocks.6.attn.q_proj.w_lora_A
transformer.resblocks.6.attn.q_proj.w_lora_B
transformer.resblocks.6.attn.k_proj.w_lora_A
transformer.resblocks.6.attn.k_proj.w_lora_B
transformer.resblocks.6.attn.v_proj.w_lora_A
transformer.resblocks.6.attn.v_proj.w_lora_B
transformer.resblocks.7.attn.q_proj.w_lora_A
transformer.resblocks.7.attn.q_proj.w_lora_B
transformer.resblocks.7.attn.k_proj.w_lora_A
transformer.resblocks.7.attn.k_proj.w_lora_B
transformer.resblocks.7.attn.v_proj.w_lora_A
transformer.resblocks.7.attn.v_proj.w_lora_B
transformer.resblocks.8.attn.q_proj.w_lora_A
transformer.resblocks.8.attn.q_proj.w_lora_B
transformer.resblocks.8.attn.k_proj.w_lora_A
transformer.resblocks.8.attn.k_proj.w_lora_B
transformer.resblocks.8.attn.v_proj.w_lora_A
transformer.resblocks.8.attn.v_proj.w_lora_B
transformer.resblocks.9.attn.q_proj.w_lora_A
transformer.resblocks.9.attn.q_proj.w_lora_B
transformer.resblocks.9.attn.k_proj.w_lora_A
transformer.resblocks.9.attn.k_proj.w_lora_B
transformer.resblocks.9.attn.v_proj.w_lora_A
transformer.resblocks.9.attn.v_proj.w_lora_B
transformer.resblocks.10.attn.q_proj.w_lora_A
transformer.resblocks.10.attn.q_proj.w_lora_B
transformer.resblocks.10.attn.k_proj.w_lora_A
transformer.resblocks.10.attn.k_proj.w_lora_B
transformer.resblocks.10.attn.v_proj.w_lora_A
transformer.resblocks.10.attn.v_proj.w_lora_B
transformer.resblocks.11.attn.q_proj.w_lora_A
transformer.resblocks.11.attn.q_proj.w_lora_B
transformer.resblocks.11.attn.k_proj.w_lora_A
transformer.resblocks.11.attn.k_proj.w_lora_B
transformer.resblocks.11.attn.v_proj.w_lora_A
transformer.resblocks.11.attn.v_proj.w_lora_B
tot=151277313, train = 552960
===============================================
[Train] Epoch: [1/1][100/120]  Time 0.165 (0.175)  cls 2.7583 (3.2084)  contrastive 0.8988 (6.2356)  loss 3.6571 (9.4440)  
epoch = [1/1]loss = {'net': 8.510877851645152, 'acc': 0.4375}

***Validation***
udcdr == 0
Query:clipart; Gallery:real; Generalized:0

Query Emb Dim:torch.Size([6394, 512]); Gallery Emb Dim:torch.Size([24387, 512])
computing unormed situation
tensor(0.5884, device='cuda:1') tensor(0.4565, device='cuda:1')
computing normed situation
tensor(0.6556, device='cuda:1') tensor(0.5641, device='cuda:1')
un-norm situation:
learned: map: 0.58836829662323, prec: 0.4565342664718628
norm situation:
learned: map: 0.6555762887001038, prec: 0.564073383808136
Query:clipart; Gallery:real; Generalized:1

Query Emb Dim:torch.Size([6394, 512]); Gallery Emb Dim:torch.Size([36600, 512])
computing unormed situation
tensor(0.5528, device='cuda:1') tensor(0.4209, device='cuda:1')
computing normed situation
tensor(0.6137, device='cuda:1') tensor(0.5225, device='cuda:1')
un-norm situation:
learned: map: 0.552847146987915, prec: 0.42091646790504456
norm situation:
learned: map: 0.6137122511863708, prec: 0.5224530696868896
udcdr == 1

Query Emb Dim:torch.Size([1180, 512]); Gallery Emb Dim:torch.Size([5857, 512])
computing unormed situation
tensor(0.5656, device='cuda:1') tensor(0.2826, device='cuda:1')
computing normed situation
tensor(0.6500, device='cuda:1') tensor(0.3797, device='cuda:1')
un-norm situation:
learned: map: 0.5656086206436157, prec: 0.2826186418533325
norm situation:
learned: map: 0.6499786376953125, prec: 0.37966522574424744
Epoch Time:2m10s lr:0.0005000 mAP:0.6137 prec:0.5225


***Training and Validation complete***
================Parameters Settings=================
Parameters:	Namespace(num_shots=8, position='all', params=['q', 'k', 'v'], r=6, alpha=1, dropout_rate=0.25, log_name='reviewer_1', encoder='both', visual='VPT', vptNumTokens=4, ctx_init='a photo of a', text='CoOp', textNumTokens=4, optimizer='adam', l2_reg=0.0, epochs=1, lr=0.0005, momentum=0.9, resume_dict=None, code_path='/home/user/Code/DePro', dataset_path='/data/UCDR/data', dataset='DomainNet', is_eccv_split=1, clip_backbone='ViT-B/32', CLS_NUM_TOKENS=300, DOM_NUM_TOKENS=5, debug_mode=0, dropout=0.5, seen_domain='painting', holdout_domain='clipart', gallery_domain='real', include_auxillary_domains=1, udcdr=0, image_size=224, seed=0, batch_size=50, num_workers=6, early_stop=2, log_interval=100)
================Training Settings=================
lr = 0.0005
batch_size = 50
==================================================
Residual Attention Block 0: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 1: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 2: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 3: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 4: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 5: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 6: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 7: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 8: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 9: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 10: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 11: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 0: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 1: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 2: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 3: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 4: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 5: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 6: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 7: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 8: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 9: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 10: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 11: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
======== Context-aware Simulator Learning Setup========
visual.transformer.resblocks.0.attn.q_proj.w_lora_A
visual.transformer.resblocks.0.attn.q_proj.w_lora_B
visual.transformer.resblocks.0.attn.k_proj.w_lora_A
visual.transformer.resblocks.0.attn.k_proj.w_lora_B
visual.transformer.resblocks.0.attn.v_proj.w_lora_A
visual.transformer.resblocks.0.attn.v_proj.w_lora_B
visual.transformer.resblocks.1.attn.q_proj.w_lora_A
visual.transformer.resblocks.1.attn.q_proj.w_lora_B
visual.transformer.resblocks.1.attn.k_proj.w_lora_A
visual.transformer.resblocks.1.attn.k_proj.w_lora_B
visual.transformer.resblocks.1.attn.v_proj.w_lora_A
visual.transformer.resblocks.1.attn.v_proj.w_lora_B
visual.transformer.resblocks.2.attn.q_proj.w_lora_A
visual.transformer.resblocks.2.attn.q_proj.w_lora_B
visual.transformer.resblocks.2.attn.k_proj.w_lora_A
visual.transformer.resblocks.2.attn.k_proj.w_lora_B
visual.transformer.resblocks.2.attn.v_proj.w_lora_A
visual.transformer.resblocks.2.attn.v_proj.w_lora_B
visual.transformer.resblocks.3.attn.q_proj.w_lora_A
visual.transformer.resblocks.3.attn.q_proj.w_lora_B
visual.transformer.resblocks.3.attn.k_proj.w_lora_A
visual.transformer.resblocks.3.attn.k_proj.w_lora_B
visual.transformer.resblocks.3.attn.v_proj.w_lora_A
visual.transformer.resblocks.3.attn.v_proj.w_lora_B
visual.transformer.resblocks.4.attn.q_proj.w_lora_A
visual.transformer.resblocks.4.attn.q_proj.w_lora_B
visual.transformer.resblocks.4.attn.k_proj.w_lora_A
visual.transformer.resblocks.4.attn.k_proj.w_lora_B
visual.transformer.resblocks.4.attn.v_proj.w_lora_A
visual.transformer.resblocks.4.attn.v_proj.w_lora_B
visual.transformer.resblocks.5.attn.q_proj.w_lora_A
visual.transformer.resblocks.5.attn.q_proj.w_lora_B
visual.transformer.resblocks.5.attn.k_proj.w_lora_A
visual.transformer.resblocks.5.attn.k_proj.w_lora_B
visual.transformer.resblocks.5.attn.v_proj.w_lora_A
visual.transformer.resblocks.5.attn.v_proj.w_lora_B
visual.transformer.resblocks.6.attn.q_proj.w_lora_A
visual.transformer.resblocks.6.attn.q_proj.w_lora_B
visual.transformer.resblocks.6.attn.k_proj.w_lora_A
visual.transformer.resblocks.6.attn.k_proj.w_lora_B
visual.transformer.resblocks.6.attn.v_proj.w_lora_A
visual.transformer.resblocks.6.attn.v_proj.w_lora_B
visual.transformer.resblocks.7.attn.q_proj.w_lora_A
visual.transformer.resblocks.7.attn.q_proj.w_lora_B
visual.transformer.resblocks.7.attn.k_proj.w_lora_A
visual.transformer.resblocks.7.attn.k_proj.w_lora_B
visual.transformer.resblocks.7.attn.v_proj.w_lora_A
visual.transformer.resblocks.7.attn.v_proj.w_lora_B
visual.transformer.resblocks.8.attn.q_proj.w_lora_A
visual.transformer.resblocks.8.attn.q_proj.w_lora_B
visual.transformer.resblocks.8.attn.k_proj.w_lora_A
visual.transformer.resblocks.8.attn.k_proj.w_lora_B
visual.transformer.resblocks.8.attn.v_proj.w_lora_A
visual.transformer.resblocks.8.attn.v_proj.w_lora_B
visual.transformer.resblocks.9.attn.q_proj.w_lora_A
visual.transformer.resblocks.9.attn.q_proj.w_lora_B
visual.transformer.resblocks.9.attn.k_proj.w_lora_A
visual.transformer.resblocks.9.attn.k_proj.w_lora_B
visual.transformer.resblocks.9.attn.v_proj.w_lora_A
visual.transformer.resblocks.9.attn.v_proj.w_lora_B
visual.transformer.resblocks.10.attn.q_proj.w_lora_A
visual.transformer.resblocks.10.attn.q_proj.w_lora_B
visual.transformer.resblocks.10.attn.k_proj.w_lora_A
visual.transformer.resblocks.10.attn.k_proj.w_lora_B
visual.transformer.resblocks.10.attn.v_proj.w_lora_A
visual.transformer.resblocks.10.attn.v_proj.w_lora_B
visual.transformer.resblocks.11.attn.q_proj.w_lora_A
visual.transformer.resblocks.11.attn.q_proj.w_lora_B
visual.transformer.resblocks.11.attn.k_proj.w_lora_A
visual.transformer.resblocks.11.attn.k_proj.w_lora_B
visual.transformer.resblocks.11.attn.v_proj.w_lora_A
visual.transformer.resblocks.11.attn.v_proj.w_lora_B
transformer.resblocks.0.attn.q_proj.w_lora_A
transformer.resblocks.0.attn.q_proj.w_lora_B
transformer.resblocks.0.attn.k_proj.w_lora_A
transformer.resblocks.0.attn.k_proj.w_lora_B
transformer.resblocks.0.attn.v_proj.w_lora_A
transformer.resblocks.0.attn.v_proj.w_lora_B
transformer.resblocks.1.attn.q_proj.w_lora_A
transformer.resblocks.1.attn.q_proj.w_lora_B
transformer.resblocks.1.attn.k_proj.w_lora_A
transformer.resblocks.1.attn.k_proj.w_lora_B
transformer.resblocks.1.attn.v_proj.w_lora_A
transformer.resblocks.1.attn.v_proj.w_lora_B
transformer.resblocks.2.attn.q_proj.w_lora_A
transformer.resblocks.2.attn.q_proj.w_lora_B
transformer.resblocks.2.attn.k_proj.w_lora_A
transformer.resblocks.2.attn.k_proj.w_lora_B
transformer.resblocks.2.attn.v_proj.w_lora_A
transformer.resblocks.2.attn.v_proj.w_lora_B
transformer.resblocks.3.attn.q_proj.w_lora_A
transformer.resblocks.3.attn.q_proj.w_lora_B
transformer.resblocks.3.attn.k_proj.w_lora_A
transformer.resblocks.3.attn.k_proj.w_lora_B
transformer.resblocks.3.attn.v_proj.w_lora_A
transformer.resblocks.3.attn.v_proj.w_lora_B
transformer.resblocks.4.attn.q_proj.w_lora_A
transformer.resblocks.4.attn.q_proj.w_lora_B
transformer.resblocks.4.attn.k_proj.w_lora_A
transformer.resblocks.4.attn.k_proj.w_lora_B
transformer.resblocks.4.attn.v_proj.w_lora_A
transformer.resblocks.4.attn.v_proj.w_lora_B
transformer.resblocks.5.attn.q_proj.w_lora_A
transformer.resblocks.5.attn.q_proj.w_lora_B
transformer.resblocks.5.attn.k_proj.w_lora_A
transformer.resblocks.5.attn.k_proj.w_lora_B
transformer.resblocks.5.attn.v_proj.w_lora_A
transformer.resblocks.5.attn.v_proj.w_lora_B
transformer.resblocks.6.attn.q_proj.w_lora_A
transformer.resblocks.6.attn.q_proj.w_lora_B
transformer.resblocks.6.attn.k_proj.w_lora_A
transformer.resblocks.6.attn.k_proj.w_lora_B
transformer.resblocks.6.attn.v_proj.w_lora_A
transformer.resblocks.6.attn.v_proj.w_lora_B
transformer.resblocks.7.attn.q_proj.w_lora_A
transformer.resblocks.7.attn.q_proj.w_lora_B
transformer.resblocks.7.attn.k_proj.w_lora_A
transformer.resblocks.7.attn.k_proj.w_lora_B
transformer.resblocks.7.attn.v_proj.w_lora_A
transformer.resblocks.7.attn.v_proj.w_lora_B
transformer.resblocks.8.attn.q_proj.w_lora_A
transformer.resblocks.8.attn.q_proj.w_lora_B
transformer.resblocks.8.attn.k_proj.w_lora_A
transformer.resblocks.8.attn.k_proj.w_lora_B
transformer.resblocks.8.attn.v_proj.w_lora_A
transformer.resblocks.8.attn.v_proj.w_lora_B
transformer.resblocks.9.attn.q_proj.w_lora_A
transformer.resblocks.9.attn.q_proj.w_lora_B
transformer.resblocks.9.attn.k_proj.w_lora_A
transformer.resblocks.9.attn.k_proj.w_lora_B
transformer.resblocks.9.attn.v_proj.w_lora_A
transformer.resblocks.9.attn.v_proj.w_lora_B
transformer.resblocks.10.attn.q_proj.w_lora_A
transformer.resblocks.10.attn.q_proj.w_lora_B
transformer.resblocks.10.attn.k_proj.w_lora_A
transformer.resblocks.10.attn.k_proj.w_lora_B
transformer.resblocks.10.attn.v_proj.w_lora_A
transformer.resblocks.10.attn.v_proj.w_lora_B
transformer.resblocks.11.attn.q_proj.w_lora_A
transformer.resblocks.11.attn.q_proj.w_lora_B
transformer.resblocks.11.attn.k_proj.w_lora_A
transformer.resblocks.11.attn.k_proj.w_lora_B
transformer.resblocks.11.attn.v_proj.w_lora_A
transformer.resblocks.11.attn.v_proj.w_lora_B
tot=151277313, train = 552960
===============================================
[Train] Epoch: [1/1][100/240]  Time 0.165 (0.175)  cls 2.3688 (3.1940)  contrastive 1.2632 (5.8600)  loss 3.6319 (9.0540)  
[Train] Epoch: [1/1][200/240]  Time 0.165 (0.170)  cls 2.3304 (2.9602)  contrastive 0.7095 (3.4324)  loss 3.0399 (6.3925)  
epoch = [1/1]loss = {'net': 5.909560612837473, 'acc': 0.4535}

***Validation***
udcdr == 0
Query:clipart; Gallery:real; Generalized:0

Query Emb Dim:torch.Size([6394, 512]); Gallery Emb Dim:torch.Size([24387, 512])
computing unormed situation
tensor(0.6241, device='cuda:1') tensor(0.5078, device='cuda:1')
computing normed situation
tensor(0.6808, device='cuda:1') tensor(0.6001, device='cuda:1')
un-norm situation:
learned: map: 0.6240529417991638, prec: 0.5077823400497437
norm situation:
learned: map: 0.6807882785797119, prec: 0.6000625491142273
Query:clipart; Gallery:real; Generalized:1

Query Emb Dim:torch.Size([6394, 512]); Gallery Emb Dim:torch.Size([36600, 512])
computing unormed situation
tensor(0.5843, device='cuda:1') tensor(0.4688, device='cuda:1')
computing normed situation
tensor(0.6362, device='cuda:1') tensor(0.5565, device='cuda:1')
un-norm situation:
learned: map: 0.5842898488044739, prec: 0.4688395857810974
norm situation:
learned: map: 0.6361870169639587, prec: 0.5565342307090759
udcdr == 1

Query Emb Dim:torch.Size([1180, 512]); Gallery Emb Dim:torch.Size([5857, 512])
computing unormed situation
tensor(0.6119, device='cuda:1') tensor(0.3206, device='cuda:1')
computing normed situation
tensor(0.6850, device='cuda:1') tensor(0.4097, device='cuda:1')
un-norm situation:
learned: map: 0.6118882298469543, prec: 0.32055509090423584
norm situation:
learned: map: 0.6850161552429199, prec: 0.40965256094932556
Epoch Time:2m29s lr:0.0005000 mAP:0.6362 prec:0.5565


***Training and Validation complete***
================Parameters Settings=================
Parameters:	Namespace(num_shots=16, position='all', params=['q', 'k', 'v'], r=6, alpha=1, dropout_rate=0.25, log_name='reviewer_1', encoder='both', visual='VPT', vptNumTokens=4, ctx_init='a photo of a', text='CoOp', textNumTokens=4, optimizer='adam', l2_reg=0.0, epochs=1, lr=0.0005, momentum=0.9, resume_dict=None, code_path='/home/user/Code/DePro', dataset_path='/data/UCDR/data', dataset='DomainNet', is_eccv_split=1, clip_backbone='ViT-B/32', CLS_NUM_TOKENS=300, DOM_NUM_TOKENS=5, debug_mode=0, dropout=0.5, seen_domain='painting', holdout_domain='clipart', gallery_domain='real', include_auxillary_domains=1, udcdr=0, image_size=224, seed=0, batch_size=50, num_workers=6, early_stop=2, log_interval=100)
================Training Settings=================
lr = 0.0005
batch_size = 50
==================================================
Residual Attention Block 0: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 1: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 2: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 3: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 4: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 5: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 6: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 7: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 8: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 9: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 10: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 11: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 0: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 1: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 2: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 3: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 4: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 5: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 6: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 7: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 8: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 9: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 10: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 11: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
======== Context-aware Simulator Learning Setup========
visual.transformer.resblocks.0.attn.q_proj.w_lora_A
visual.transformer.resblocks.0.attn.q_proj.w_lora_B
visual.transformer.resblocks.0.attn.k_proj.w_lora_A
visual.transformer.resblocks.0.attn.k_proj.w_lora_B
visual.transformer.resblocks.0.attn.v_proj.w_lora_A
visual.transformer.resblocks.0.attn.v_proj.w_lora_B
visual.transformer.resblocks.1.attn.q_proj.w_lora_A
visual.transformer.resblocks.1.attn.q_proj.w_lora_B
visual.transformer.resblocks.1.attn.k_proj.w_lora_A
visual.transformer.resblocks.1.attn.k_proj.w_lora_B
visual.transformer.resblocks.1.attn.v_proj.w_lora_A
visual.transformer.resblocks.1.attn.v_proj.w_lora_B
visual.transformer.resblocks.2.attn.q_proj.w_lora_A
visual.transformer.resblocks.2.attn.q_proj.w_lora_B
visual.transformer.resblocks.2.attn.k_proj.w_lora_A
visual.transformer.resblocks.2.attn.k_proj.w_lora_B
visual.transformer.resblocks.2.attn.v_proj.w_lora_A
visual.transformer.resblocks.2.attn.v_proj.w_lora_B
visual.transformer.resblocks.3.attn.q_proj.w_lora_A
visual.transformer.resblocks.3.attn.q_proj.w_lora_B
visual.transformer.resblocks.3.attn.k_proj.w_lora_A
visual.transformer.resblocks.3.attn.k_proj.w_lora_B
visual.transformer.resblocks.3.attn.v_proj.w_lora_A
visual.transformer.resblocks.3.attn.v_proj.w_lora_B
visual.transformer.resblocks.4.attn.q_proj.w_lora_A
visual.transformer.resblocks.4.attn.q_proj.w_lora_B
visual.transformer.resblocks.4.attn.k_proj.w_lora_A
visual.transformer.resblocks.4.attn.k_proj.w_lora_B
visual.transformer.resblocks.4.attn.v_proj.w_lora_A
visual.transformer.resblocks.4.attn.v_proj.w_lora_B
visual.transformer.resblocks.5.attn.q_proj.w_lora_A
visual.transformer.resblocks.5.attn.q_proj.w_lora_B
visual.transformer.resblocks.5.attn.k_proj.w_lora_A
visual.transformer.resblocks.5.attn.k_proj.w_lora_B
visual.transformer.resblocks.5.attn.v_proj.w_lora_A
visual.transformer.resblocks.5.attn.v_proj.w_lora_B
visual.transformer.resblocks.6.attn.q_proj.w_lora_A
visual.transformer.resblocks.6.attn.q_proj.w_lora_B
visual.transformer.resblocks.6.attn.k_proj.w_lora_A
visual.transformer.resblocks.6.attn.k_proj.w_lora_B
visual.transformer.resblocks.6.attn.v_proj.w_lora_A
visual.transformer.resblocks.6.attn.v_proj.w_lora_B
visual.transformer.resblocks.7.attn.q_proj.w_lora_A
visual.transformer.resblocks.7.attn.q_proj.w_lora_B
visual.transformer.resblocks.7.attn.k_proj.w_lora_A
visual.transformer.resblocks.7.attn.k_proj.w_lora_B
visual.transformer.resblocks.7.attn.v_proj.w_lora_A
visual.transformer.resblocks.7.attn.v_proj.w_lora_B
visual.transformer.resblocks.8.attn.q_proj.w_lora_A
visual.transformer.resblocks.8.attn.q_proj.w_lora_B
visual.transformer.resblocks.8.attn.k_proj.w_lora_A
visual.transformer.resblocks.8.attn.k_proj.w_lora_B
visual.transformer.resblocks.8.attn.v_proj.w_lora_A
visual.transformer.resblocks.8.attn.v_proj.w_lora_B
visual.transformer.resblocks.9.attn.q_proj.w_lora_A
visual.transformer.resblocks.9.attn.q_proj.w_lora_B
visual.transformer.resblocks.9.attn.k_proj.w_lora_A
visual.transformer.resblocks.9.attn.k_proj.w_lora_B
visual.transformer.resblocks.9.attn.v_proj.w_lora_A
visual.transformer.resblocks.9.attn.v_proj.w_lora_B
visual.transformer.resblocks.10.attn.q_proj.w_lora_A
visual.transformer.resblocks.10.attn.q_proj.w_lora_B
visual.transformer.resblocks.10.attn.k_proj.w_lora_A
visual.transformer.resblocks.10.attn.k_proj.w_lora_B
visual.transformer.resblocks.10.attn.v_proj.w_lora_A
visual.transformer.resblocks.10.attn.v_proj.w_lora_B
visual.transformer.resblocks.11.attn.q_proj.w_lora_A
visual.transformer.resblocks.11.attn.q_proj.w_lora_B
visual.transformer.resblocks.11.attn.k_proj.w_lora_A
visual.transformer.resblocks.11.attn.k_proj.w_lora_B
visual.transformer.resblocks.11.attn.v_proj.w_lora_A
visual.transformer.resblocks.11.attn.v_proj.w_lora_B
transformer.resblocks.0.attn.q_proj.w_lora_A
transformer.resblocks.0.attn.q_proj.w_lora_B
transformer.resblocks.0.attn.k_proj.w_lora_A
transformer.resblocks.0.attn.k_proj.w_lora_B
transformer.resblocks.0.attn.v_proj.w_lora_A
transformer.resblocks.0.attn.v_proj.w_lora_B
transformer.resblocks.1.attn.q_proj.w_lora_A
transformer.resblocks.1.attn.q_proj.w_lora_B
transformer.resblocks.1.attn.k_proj.w_lora_A
transformer.resblocks.1.attn.k_proj.w_lora_B
transformer.resblocks.1.attn.v_proj.w_lora_A
transformer.resblocks.1.attn.v_proj.w_lora_B
transformer.resblocks.2.attn.q_proj.w_lora_A
transformer.resblocks.2.attn.q_proj.w_lora_B
transformer.resblocks.2.attn.k_proj.w_lora_A
transformer.resblocks.2.attn.k_proj.w_lora_B
transformer.resblocks.2.attn.v_proj.w_lora_A
transformer.resblocks.2.attn.v_proj.w_lora_B
transformer.resblocks.3.attn.q_proj.w_lora_A
transformer.resblocks.3.attn.q_proj.w_lora_B
transformer.resblocks.3.attn.k_proj.w_lora_A
transformer.resblocks.3.attn.k_proj.w_lora_B
transformer.resblocks.3.attn.v_proj.w_lora_A
transformer.resblocks.3.attn.v_proj.w_lora_B
transformer.resblocks.4.attn.q_proj.w_lora_A
transformer.resblocks.4.attn.q_proj.w_lora_B
transformer.resblocks.4.attn.k_proj.w_lora_A
transformer.resblocks.4.attn.k_proj.w_lora_B
transformer.resblocks.4.attn.v_proj.w_lora_A
transformer.resblocks.4.attn.v_proj.w_lora_B
transformer.resblocks.5.attn.q_proj.w_lora_A
transformer.resblocks.5.attn.q_proj.w_lora_B
transformer.resblocks.5.attn.k_proj.w_lora_A
transformer.resblocks.5.attn.k_proj.w_lora_B
transformer.resblocks.5.attn.v_proj.w_lora_A
transformer.resblocks.5.attn.v_proj.w_lora_B
transformer.resblocks.6.attn.q_proj.w_lora_A
transformer.resblocks.6.attn.q_proj.w_lora_B
transformer.resblocks.6.attn.k_proj.w_lora_A
transformer.resblocks.6.attn.k_proj.w_lora_B
transformer.resblocks.6.attn.v_proj.w_lora_A
transformer.resblocks.6.attn.v_proj.w_lora_B
transformer.resblocks.7.attn.q_proj.w_lora_A
transformer.resblocks.7.attn.q_proj.w_lora_B
transformer.resblocks.7.attn.k_proj.w_lora_A
transformer.resblocks.7.attn.k_proj.w_lora_B
transformer.resblocks.7.attn.v_proj.w_lora_A
transformer.resblocks.7.attn.v_proj.w_lora_B
transformer.resblocks.8.attn.q_proj.w_lora_A
transformer.resblocks.8.attn.q_proj.w_lora_B
transformer.resblocks.8.attn.k_proj.w_lora_A
transformer.resblocks.8.attn.k_proj.w_lora_B
transformer.resblocks.8.attn.v_proj.w_lora_A
transformer.resblocks.8.attn.v_proj.w_lora_B
transformer.resblocks.9.attn.q_proj.w_lora_A
transformer.resblocks.9.attn.q_proj.w_lora_B
transformer.resblocks.9.attn.k_proj.w_lora_A
transformer.resblocks.9.attn.k_proj.w_lora_B
transformer.resblocks.9.attn.v_proj.w_lora_A
transformer.resblocks.9.attn.v_proj.w_lora_B
transformer.resblocks.10.attn.q_proj.w_lora_A
transformer.resblocks.10.attn.q_proj.w_lora_B
transformer.resblocks.10.attn.k_proj.w_lora_A
transformer.resblocks.10.attn.k_proj.w_lora_B
transformer.resblocks.10.attn.v_proj.w_lora_A
transformer.resblocks.10.attn.v_proj.w_lora_B
transformer.resblocks.11.attn.q_proj.w_lora_A
transformer.resblocks.11.attn.q_proj.w_lora_B
transformer.resblocks.11.attn.k_proj.w_lora_A
transformer.resblocks.11.attn.k_proj.w_lora_B
transformer.resblocks.11.attn.v_proj.w_lora_A
transformer.resblocks.11.attn.v_proj.w_lora_B
tot=151277313, train = 552960
===============================================
[Train] Epoch: [1/1][100/473]  Time 0.165 (0.175)  cls 2.4582 (3.1873)  contrastive 1.3699 (6.1167)  loss 3.8281 (9.3039)  
[Train] Epoch: [1/1][200/473]  Time 0.165 (0.170)  cls 2.9288 (2.9345)  contrastive 1.1149 (3.5656)  loss 4.0437 (6.5002)  
[Train] Epoch: [1/1][300/473]  Time 0.165 (0.168)  cls 1.7727 (2.8214)  contrastive 0.6110 (2.6710)  loss 2.3837 (5.4924)  
[Train] Epoch: [1/1][400/473]  Time 0.165 (0.168)  cls 2.5266 (2.7505)  contrastive 1.4369 (2.2116)  loss 3.9635 (4.9621)  
epoch = [1/1]loss = {'net': 4.69677714491657, 'acc': 0.46916243654822337}

***Validation***
udcdr == 0
Query:clipart; Gallery:real; Generalized:0

Query Emb Dim:torch.Size([6394, 512]); Gallery Emb Dim:torch.Size([24387, 512])
computing unormed situation
tensor(0.6462, device='cuda:1') tensor(0.5382, device='cuda:1')
computing normed situation
tensor(0.6934, device='cuda:1') tensor(0.6160, device='cuda:1')
un-norm situation:
learned: map: 0.6462122797966003, prec: 0.5382217764854431
norm situation:
learned: map: 0.6934404969215393, prec: 0.6160157918930054
Query:clipart; Gallery:real; Generalized:1

Query Emb Dim:torch.Size([6394, 512]); Gallery Emb Dim:torch.Size([36600, 512])
computing unormed situation
tensor(0.6049, device='cuda:1') tensor(0.4989, device='cuda:1')
computing normed situation
tensor(0.6483, device='cuda:1') tensor(0.5733, device='cuda:1')
un-norm situation:
learned: map: 0.6049056649208069, prec: 0.49890366196632385
norm situation:
learned: map: 0.6483497023582458, prec: 0.5732678771018982
udcdr == 1

Query Emb Dim:torch.Size([1180, 512]); Gallery Emb Dim:torch.Size([5857, 512])
computing unormed situation
tensor(0.6487, device='cuda:1') tensor(0.3590, device='cuda:1')
computing normed situation
tensor(0.7129, device='cuda:1') tensor(0.4344, device='cuda:1')
un-norm situation:
learned: map: 0.6486753225326538, prec: 0.3589576184749603
norm situation:
learned: map: 0.7128742337226868, prec: 0.434427946805954
Epoch Time:3m8s lr:0.0005000 mAP:0.6483 prec:0.5733


***Training and Validation complete***
