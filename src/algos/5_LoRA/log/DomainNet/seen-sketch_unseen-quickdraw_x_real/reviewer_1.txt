================Parameters Settings=================
Parameters:	Namespace(num_shots=1, position='all', params=['q', 'k', 'v'], r=6, alpha=1, dropout_rate=0.25, log_name='reviewer_1', encoder='both', visual='VPT', vptNumTokens=4, ctx_init='a photo of a', text='CoOp', textNumTokens=4, optimizer='adam', l2_reg=0.0, epochs=1, lr=0.0005, momentum=0.9, resume_dict=None, code_path='/home/user/Code/DePro', dataset_path='/data/UCDR/data', dataset='DomainNet', is_eccv_split=1, clip_backbone='ViT-B/32', CLS_NUM_TOKENS=300, DOM_NUM_TOKENS=5, debug_mode=0, dropout=0.5, seen_domain='sketch', holdout_domain='quickdraw', gallery_domain='real', include_auxillary_domains=1, udcdr=0, image_size=224, seed=0, batch_size=50, num_workers=6, early_stop=2, log_interval=100)
================Training Settings=================
lr = 0.0005
batch_size = 50
==================================================
Residual Attention Block 0: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 1: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 2: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 3: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 4: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 5: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 6: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 7: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 8: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 9: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 10: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 11: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 0: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 1: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 2: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 3: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 4: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 5: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 6: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 7: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 8: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 9: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 10: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 11: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
======== Context-aware Simulator Learning Setup========
visual.transformer.resblocks.0.attn.q_proj.w_lora_A
visual.transformer.resblocks.0.attn.q_proj.w_lora_B
visual.transformer.resblocks.0.attn.k_proj.w_lora_A
visual.transformer.resblocks.0.attn.k_proj.w_lora_B
visual.transformer.resblocks.0.attn.v_proj.w_lora_A
visual.transformer.resblocks.0.attn.v_proj.w_lora_B
visual.transformer.resblocks.1.attn.q_proj.w_lora_A
visual.transformer.resblocks.1.attn.q_proj.w_lora_B
visual.transformer.resblocks.1.attn.k_proj.w_lora_A
visual.transformer.resblocks.1.attn.k_proj.w_lora_B
visual.transformer.resblocks.1.attn.v_proj.w_lora_A
visual.transformer.resblocks.1.attn.v_proj.w_lora_B
visual.transformer.resblocks.2.attn.q_proj.w_lora_A
visual.transformer.resblocks.2.attn.q_proj.w_lora_B
visual.transformer.resblocks.2.attn.k_proj.w_lora_A
visual.transformer.resblocks.2.attn.k_proj.w_lora_B
visual.transformer.resblocks.2.attn.v_proj.w_lora_A
visual.transformer.resblocks.2.attn.v_proj.w_lora_B
visual.transformer.resblocks.3.attn.q_proj.w_lora_A
visual.transformer.resblocks.3.attn.q_proj.w_lora_B
visual.transformer.resblocks.3.attn.k_proj.w_lora_A
visual.transformer.resblocks.3.attn.k_proj.w_lora_B
visual.transformer.resblocks.3.attn.v_proj.w_lora_A
visual.transformer.resblocks.3.attn.v_proj.w_lora_B
visual.transformer.resblocks.4.attn.q_proj.w_lora_A
visual.transformer.resblocks.4.attn.q_proj.w_lora_B
visual.transformer.resblocks.4.attn.k_proj.w_lora_A
visual.transformer.resblocks.4.attn.k_proj.w_lora_B
visual.transformer.resblocks.4.attn.v_proj.w_lora_A
visual.transformer.resblocks.4.attn.v_proj.w_lora_B
visual.transformer.resblocks.5.attn.q_proj.w_lora_A
visual.transformer.resblocks.5.attn.q_proj.w_lora_B
visual.transformer.resblocks.5.attn.k_proj.w_lora_A
visual.transformer.resblocks.5.attn.k_proj.w_lora_B
visual.transformer.resblocks.5.attn.v_proj.w_lora_A
visual.transformer.resblocks.5.attn.v_proj.w_lora_B
visual.transformer.resblocks.6.attn.q_proj.w_lora_A
visual.transformer.resblocks.6.attn.q_proj.w_lora_B
visual.transformer.resblocks.6.attn.k_proj.w_lora_A
visual.transformer.resblocks.6.attn.k_proj.w_lora_B
visual.transformer.resblocks.6.attn.v_proj.w_lora_A
visual.transformer.resblocks.6.attn.v_proj.w_lora_B
visual.transformer.resblocks.7.attn.q_proj.w_lora_A
visual.transformer.resblocks.7.attn.q_proj.w_lora_B
visual.transformer.resblocks.7.attn.k_proj.w_lora_A
visual.transformer.resblocks.7.attn.k_proj.w_lora_B
visual.transformer.resblocks.7.attn.v_proj.w_lora_A
visual.transformer.resblocks.7.attn.v_proj.w_lora_B
visual.transformer.resblocks.8.attn.q_proj.w_lora_A
visual.transformer.resblocks.8.attn.q_proj.w_lora_B
visual.transformer.resblocks.8.attn.k_proj.w_lora_A
visual.transformer.resblocks.8.attn.k_proj.w_lora_B
visual.transformer.resblocks.8.attn.v_proj.w_lora_A
visual.transformer.resblocks.8.attn.v_proj.w_lora_B
visual.transformer.resblocks.9.attn.q_proj.w_lora_A
visual.transformer.resblocks.9.attn.q_proj.w_lora_B
visual.transformer.resblocks.9.attn.k_proj.w_lora_A
visual.transformer.resblocks.9.attn.k_proj.w_lora_B
visual.transformer.resblocks.9.attn.v_proj.w_lora_A
visual.transformer.resblocks.9.attn.v_proj.w_lora_B
visual.transformer.resblocks.10.attn.q_proj.w_lora_A
visual.transformer.resblocks.10.attn.q_proj.w_lora_B
visual.transformer.resblocks.10.attn.k_proj.w_lora_A
visual.transformer.resblocks.10.attn.k_proj.w_lora_B
visual.transformer.resblocks.10.attn.v_proj.w_lora_A
visual.transformer.resblocks.10.attn.v_proj.w_lora_B
visual.transformer.resblocks.11.attn.q_proj.w_lora_A
visual.transformer.resblocks.11.attn.q_proj.w_lora_B
visual.transformer.resblocks.11.attn.k_proj.w_lora_A
visual.transformer.resblocks.11.attn.k_proj.w_lora_B
visual.transformer.resblocks.11.attn.v_proj.w_lora_A
visual.transformer.resblocks.11.attn.v_proj.w_lora_B
transformer.resblocks.0.attn.q_proj.w_lora_A
transformer.resblocks.0.attn.q_proj.w_lora_B
transformer.resblocks.0.attn.k_proj.w_lora_A
transformer.resblocks.0.attn.k_proj.w_lora_B
transformer.resblocks.0.attn.v_proj.w_lora_A
transformer.resblocks.0.attn.v_proj.w_lora_B
transformer.resblocks.1.attn.q_proj.w_lora_A
transformer.resblocks.1.attn.q_proj.w_lora_B
transformer.resblocks.1.attn.k_proj.w_lora_A
transformer.resblocks.1.attn.k_proj.w_lora_B
transformer.resblocks.1.attn.v_proj.w_lora_A
transformer.resblocks.1.attn.v_proj.w_lora_B
transformer.resblocks.2.attn.q_proj.w_lora_A
transformer.resblocks.2.attn.q_proj.w_lora_B
transformer.resblocks.2.attn.k_proj.w_lora_A
transformer.resblocks.2.attn.k_proj.w_lora_B
transformer.resblocks.2.attn.v_proj.w_lora_A
transformer.resblocks.2.attn.v_proj.w_lora_B
transformer.resblocks.3.attn.q_proj.w_lora_A
transformer.resblocks.3.attn.q_proj.w_lora_B
transformer.resblocks.3.attn.k_proj.w_lora_A
transformer.resblocks.3.attn.k_proj.w_lora_B
transformer.resblocks.3.attn.v_proj.w_lora_A
transformer.resblocks.3.attn.v_proj.w_lora_B
transformer.resblocks.4.attn.q_proj.w_lora_A
transformer.resblocks.4.attn.q_proj.w_lora_B
transformer.resblocks.4.attn.k_proj.w_lora_A
transformer.resblocks.4.attn.k_proj.w_lora_B
transformer.resblocks.4.attn.v_proj.w_lora_A
transformer.resblocks.4.attn.v_proj.w_lora_B
transformer.resblocks.5.attn.q_proj.w_lora_A
transformer.resblocks.5.attn.q_proj.w_lora_B
transformer.resblocks.5.attn.k_proj.w_lora_A
transformer.resblocks.5.attn.k_proj.w_lora_B
transformer.resblocks.5.attn.v_proj.w_lora_A
transformer.resblocks.5.attn.v_proj.w_lora_B
transformer.resblocks.6.attn.q_proj.w_lora_A
transformer.resblocks.6.attn.q_proj.w_lora_B
transformer.resblocks.6.attn.k_proj.w_lora_A
transformer.resblocks.6.attn.k_proj.w_lora_B
transformer.resblocks.6.attn.v_proj.w_lora_A
transformer.resblocks.6.attn.v_proj.w_lora_B
transformer.resblocks.7.attn.q_proj.w_lora_A
transformer.resblocks.7.attn.q_proj.w_lora_B
transformer.resblocks.7.attn.k_proj.w_lora_A
transformer.resblocks.7.attn.k_proj.w_lora_B
transformer.resblocks.7.attn.v_proj.w_lora_A
transformer.resblocks.7.attn.v_proj.w_lora_B
transformer.resblocks.8.attn.q_proj.w_lora_A
transformer.resblocks.8.attn.q_proj.w_lora_B
transformer.resblocks.8.attn.k_proj.w_lora_A
transformer.resblocks.8.attn.k_proj.w_lora_B
transformer.resblocks.8.attn.v_proj.w_lora_A
transformer.resblocks.8.attn.v_proj.w_lora_B
transformer.resblocks.9.attn.q_proj.w_lora_A
transformer.resblocks.9.attn.q_proj.w_lora_B
transformer.resblocks.9.attn.k_proj.w_lora_A
transformer.resblocks.9.attn.k_proj.w_lora_B
transformer.resblocks.9.attn.v_proj.w_lora_A
transformer.resblocks.9.attn.v_proj.w_lora_B
transformer.resblocks.10.attn.q_proj.w_lora_A
transformer.resblocks.10.attn.q_proj.w_lora_B
transformer.resblocks.10.attn.k_proj.w_lora_A
transformer.resblocks.10.attn.k_proj.w_lora_B
transformer.resblocks.10.attn.v_proj.w_lora_A
transformer.resblocks.10.attn.v_proj.w_lora_B
transformer.resblocks.11.attn.q_proj.w_lora_A
transformer.resblocks.11.attn.q_proj.w_lora_B
transformer.resblocks.11.attn.k_proj.w_lora_A
transformer.resblocks.11.attn.k_proj.w_lora_B
transformer.resblocks.11.attn.v_proj.w_lora_A
transformer.resblocks.11.attn.v_proj.w_lora_B
tot=151277313, train = 552960
===============================================
epoch = [1/1]loss = {'net': 3.6912261327107747, 'acc': 0.5606666666666666}

***Validation***
udcdr == 0
Query:quickdraw; Gallery:real; Generalized:0

Query Emb Dim:torch.Size([22500, 512]); Gallery Emb Dim:torch.Size([24387, 512])
computing unormed situation
tensor(0.1211, device='cuda:1') tensor(0.1001, device='cuda:1')
computing normed situation
tensor(0.1122, device='cuda:1') tensor(0.0885, device='cuda:1')
un-norm situation:
learned: map: 0.12106386572122574, prec: 0.10009133070707321
norm situation:
learned: map: 0.11217773705720901, prec: 0.08850111067295074
Query:quickdraw; Gallery:real; Generalized:1

Query Emb Dim:torch.Size([22500, 512]); Gallery Emb Dim:torch.Size([36600, 512])
computing unormed situation
tensor(0.0926, device='cuda:1') tensor(0.0639, device='cuda:1')
computing normed situation
tensor(0.0859, device='cuda:1') tensor(0.0588, device='cuda:1')
un-norm situation:
learned: map: 0.09258908778429031, prec: 0.06387711316347122
norm situation:
learned: map: 0.08585766702890396, prec: 0.05875999853014946
udcdr == 1

Query Emb Dim:torch.Size([2250, 512]); Gallery Emb Dim:torch.Size([2332, 512])
computing unormed situation
tensor(0.1177, device='cuda:1') tensor(0.0574, device='cuda:1')
computing normed situation
tensor(0.1166, device='cuda:1') tensor(0.0535, device='cuda:1')
un-norm situation:
learned: map: 0.11773724853992462, prec: 0.05742666497826576
norm situation:
learned: map: 0.11655494570732117, prec: 0.05350666865706444
Epoch Time:4m12s lr:0.0005000 mAP:0.0859 prec:0.0588


***Training and Validation complete***
================Parameters Settings=================
Parameters:	Namespace(num_shots=2, position='all', params=['q', 'k', 'v'], r=6, alpha=1, dropout_rate=0.25, log_name='reviewer_1', encoder='both', visual='VPT', vptNumTokens=4, ctx_init='a photo of a', text='CoOp', textNumTokens=4, optimizer='adam', l2_reg=0.0, epochs=1, lr=0.0005, momentum=0.9, resume_dict=None, code_path='/home/user/Code/DePro', dataset_path='/data/UCDR/data', dataset='DomainNet', is_eccv_split=1, clip_backbone='ViT-B/32', CLS_NUM_TOKENS=300, DOM_NUM_TOKENS=5, debug_mode=0, dropout=0.5, seen_domain='sketch', holdout_domain='quickdraw', gallery_domain='real', include_auxillary_domains=1, udcdr=0, image_size=224, seed=0, batch_size=50, num_workers=6, early_stop=2, log_interval=100)
================Training Settings=================
lr = 0.0005
batch_size = 50
==================================================
Residual Attention Block 0: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 1: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 2: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 3: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 4: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 5: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 6: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 7: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 8: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 9: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 10: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 11: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 0: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 1: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 2: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 3: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 4: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 5: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 6: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 7: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 8: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 9: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 10: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 11: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
======== Context-aware Simulator Learning Setup========
visual.transformer.resblocks.0.attn.q_proj.w_lora_A
visual.transformer.resblocks.0.attn.q_proj.w_lora_B
visual.transformer.resblocks.0.attn.k_proj.w_lora_A
visual.transformer.resblocks.0.attn.k_proj.w_lora_B
visual.transformer.resblocks.0.attn.v_proj.w_lora_A
visual.transformer.resblocks.0.attn.v_proj.w_lora_B
visual.transformer.resblocks.1.attn.q_proj.w_lora_A
visual.transformer.resblocks.1.attn.q_proj.w_lora_B
visual.transformer.resblocks.1.attn.k_proj.w_lora_A
visual.transformer.resblocks.1.attn.k_proj.w_lora_B
visual.transformer.resblocks.1.attn.v_proj.w_lora_A
visual.transformer.resblocks.1.attn.v_proj.w_lora_B
visual.transformer.resblocks.2.attn.q_proj.w_lora_A
visual.transformer.resblocks.2.attn.q_proj.w_lora_B
visual.transformer.resblocks.2.attn.k_proj.w_lora_A
visual.transformer.resblocks.2.attn.k_proj.w_lora_B
visual.transformer.resblocks.2.attn.v_proj.w_lora_A
visual.transformer.resblocks.2.attn.v_proj.w_lora_B
visual.transformer.resblocks.3.attn.q_proj.w_lora_A
visual.transformer.resblocks.3.attn.q_proj.w_lora_B
visual.transformer.resblocks.3.attn.k_proj.w_lora_A
visual.transformer.resblocks.3.attn.k_proj.w_lora_B
visual.transformer.resblocks.3.attn.v_proj.w_lora_A
visual.transformer.resblocks.3.attn.v_proj.w_lora_B
visual.transformer.resblocks.4.attn.q_proj.w_lora_A
visual.transformer.resblocks.4.attn.q_proj.w_lora_B
visual.transformer.resblocks.4.attn.k_proj.w_lora_A
visual.transformer.resblocks.4.attn.k_proj.w_lora_B
visual.transformer.resblocks.4.attn.v_proj.w_lora_A
visual.transformer.resblocks.4.attn.v_proj.w_lora_B
visual.transformer.resblocks.5.attn.q_proj.w_lora_A
visual.transformer.resblocks.5.attn.q_proj.w_lora_B
visual.transformer.resblocks.5.attn.k_proj.w_lora_A
visual.transformer.resblocks.5.attn.k_proj.w_lora_B
visual.transformer.resblocks.5.attn.v_proj.w_lora_A
visual.transformer.resblocks.5.attn.v_proj.w_lora_B
visual.transformer.resblocks.6.attn.q_proj.w_lora_A
visual.transformer.resblocks.6.attn.q_proj.w_lora_B
visual.transformer.resblocks.6.attn.k_proj.w_lora_A
visual.transformer.resblocks.6.attn.k_proj.w_lora_B
visual.transformer.resblocks.6.attn.v_proj.w_lora_A
visual.transformer.resblocks.6.attn.v_proj.w_lora_B
visual.transformer.resblocks.7.attn.q_proj.w_lora_A
visual.transformer.resblocks.7.attn.q_proj.w_lora_B
visual.transformer.resblocks.7.attn.k_proj.w_lora_A
visual.transformer.resblocks.7.attn.k_proj.w_lora_B
visual.transformer.resblocks.7.attn.v_proj.w_lora_A
visual.transformer.resblocks.7.attn.v_proj.w_lora_B
visual.transformer.resblocks.8.attn.q_proj.w_lora_A
visual.transformer.resblocks.8.attn.q_proj.w_lora_B
visual.transformer.resblocks.8.attn.k_proj.w_lora_A
visual.transformer.resblocks.8.attn.k_proj.w_lora_B
visual.transformer.resblocks.8.attn.v_proj.w_lora_A
visual.transformer.resblocks.8.attn.v_proj.w_lora_B
visual.transformer.resblocks.9.attn.q_proj.w_lora_A
visual.transformer.resblocks.9.attn.q_proj.w_lora_B
visual.transformer.resblocks.9.attn.k_proj.w_lora_A
visual.transformer.resblocks.9.attn.k_proj.w_lora_B
visual.transformer.resblocks.9.attn.v_proj.w_lora_A
visual.transformer.resblocks.9.attn.v_proj.w_lora_B
visual.transformer.resblocks.10.attn.q_proj.w_lora_A
visual.transformer.resblocks.10.attn.q_proj.w_lora_B
visual.transformer.resblocks.10.attn.k_proj.w_lora_A
visual.transformer.resblocks.10.attn.k_proj.w_lora_B
visual.transformer.resblocks.10.attn.v_proj.w_lora_A
visual.transformer.resblocks.10.attn.v_proj.w_lora_B
visual.transformer.resblocks.11.attn.q_proj.w_lora_A
visual.transformer.resblocks.11.attn.q_proj.w_lora_B
visual.transformer.resblocks.11.attn.k_proj.w_lora_A
visual.transformer.resblocks.11.attn.k_proj.w_lora_B
visual.transformer.resblocks.11.attn.v_proj.w_lora_A
visual.transformer.resblocks.11.attn.v_proj.w_lora_B
transformer.resblocks.0.attn.q_proj.w_lora_A
transformer.resblocks.0.attn.q_proj.w_lora_B
transformer.resblocks.0.attn.k_proj.w_lora_A
transformer.resblocks.0.attn.k_proj.w_lora_B
transformer.resblocks.0.attn.v_proj.w_lora_A
transformer.resblocks.0.attn.v_proj.w_lora_B
transformer.resblocks.1.attn.q_proj.w_lora_A
transformer.resblocks.1.attn.q_proj.w_lora_B
transformer.resblocks.1.attn.k_proj.w_lora_A
transformer.resblocks.1.attn.k_proj.w_lora_B
transformer.resblocks.1.attn.v_proj.w_lora_A
transformer.resblocks.1.attn.v_proj.w_lora_B
transformer.resblocks.2.attn.q_proj.w_lora_A
transformer.resblocks.2.attn.q_proj.w_lora_B
transformer.resblocks.2.attn.k_proj.w_lora_A
transformer.resblocks.2.attn.k_proj.w_lora_B
transformer.resblocks.2.attn.v_proj.w_lora_A
transformer.resblocks.2.attn.v_proj.w_lora_B
transformer.resblocks.3.attn.q_proj.w_lora_A
transformer.resblocks.3.attn.q_proj.w_lora_B
transformer.resblocks.3.attn.k_proj.w_lora_A
transformer.resblocks.3.attn.k_proj.w_lora_B
transformer.resblocks.3.attn.v_proj.w_lora_A
transformer.resblocks.3.attn.v_proj.w_lora_B
transformer.resblocks.4.attn.q_proj.w_lora_A
transformer.resblocks.4.attn.q_proj.w_lora_B
transformer.resblocks.4.attn.k_proj.w_lora_A
transformer.resblocks.4.attn.k_proj.w_lora_B
transformer.resblocks.4.attn.v_proj.w_lora_A
transformer.resblocks.4.attn.v_proj.w_lora_B
transformer.resblocks.5.attn.q_proj.w_lora_A
transformer.resblocks.5.attn.q_proj.w_lora_B
transformer.resblocks.5.attn.k_proj.w_lora_A
transformer.resblocks.5.attn.k_proj.w_lora_B
transformer.resblocks.5.attn.v_proj.w_lora_A
transformer.resblocks.5.attn.v_proj.w_lora_B
transformer.resblocks.6.attn.q_proj.w_lora_A
transformer.resblocks.6.attn.q_proj.w_lora_B
transformer.resblocks.6.attn.k_proj.w_lora_A
transformer.resblocks.6.attn.k_proj.w_lora_B
transformer.resblocks.6.attn.v_proj.w_lora_A
transformer.resblocks.6.attn.v_proj.w_lora_B
transformer.resblocks.7.attn.q_proj.w_lora_A
transformer.resblocks.7.attn.q_proj.w_lora_B
transformer.resblocks.7.attn.k_proj.w_lora_A
transformer.resblocks.7.attn.k_proj.w_lora_B
transformer.resblocks.7.attn.v_proj.w_lora_A
transformer.resblocks.7.attn.v_proj.w_lora_B
transformer.resblocks.8.attn.q_proj.w_lora_A
transformer.resblocks.8.attn.q_proj.w_lora_B
transformer.resblocks.8.attn.k_proj.w_lora_A
transformer.resblocks.8.attn.k_proj.w_lora_B
transformer.resblocks.8.attn.v_proj.w_lora_A
transformer.resblocks.8.attn.v_proj.w_lora_B
transformer.resblocks.9.attn.q_proj.w_lora_A
transformer.resblocks.9.attn.q_proj.w_lora_B
transformer.resblocks.9.attn.k_proj.w_lora_A
transformer.resblocks.9.attn.k_proj.w_lora_B
transformer.resblocks.9.attn.v_proj.w_lora_A
transformer.resblocks.9.attn.v_proj.w_lora_B
transformer.resblocks.10.attn.q_proj.w_lora_A
transformer.resblocks.10.attn.q_proj.w_lora_B
transformer.resblocks.10.attn.k_proj.w_lora_A
transformer.resblocks.10.attn.k_proj.w_lora_B
transformer.resblocks.10.attn.v_proj.w_lora_A
transformer.resblocks.10.attn.v_proj.w_lora_B
transformer.resblocks.11.attn.q_proj.w_lora_A
transformer.resblocks.11.attn.q_proj.w_lora_B
transformer.resblocks.11.attn.k_proj.w_lora_A
transformer.resblocks.11.attn.k_proj.w_lora_B
transformer.resblocks.11.attn.v_proj.w_lora_A
transformer.resblocks.11.attn.v_proj.w_lora_B
tot=151277313, train = 552960
===============================================
epoch = [1/1]loss = {'net': 8.377099816004435, 'acc': 0.56}

***Validation***
udcdr == 0
Query:quickdraw; Gallery:real; Generalized:0

Query Emb Dim:torch.Size([22500, 512]); Gallery Emb Dim:torch.Size([24387, 512])
computing unormed situation
tensor(0.1645, device='cuda:1') tensor(0.1210, device='cuda:1')
computing normed situation
tensor(0.2216, device='cuda:1') tensor(0.1818, device='cuda:1')
un-norm situation:
learned: map: 0.16448120772838593, prec: 0.12102866917848587
norm situation:
learned: map: 0.22157134115695953, prec: 0.18183933198451996
Query:quickdraw; Gallery:real; Generalized:1

Query Emb Dim:torch.Size([22500, 512]); Gallery Emb Dim:torch.Size([36600, 512])
computing unormed situation
tensor(0.1336, device='cuda:1') tensor(0.0916, device='cuda:1')
computing normed situation
tensor(0.1773, device='cuda:1') tensor(0.1421, device='cuda:1')
un-norm situation:
learned: map: 0.1336105316877365, prec: 0.09158466011285782
norm situation:
learned: map: 0.1772507131099701, prec: 0.1421293318271637
udcdr == 1

Query Emb Dim:torch.Size([2250, 512]); Gallery Emb Dim:torch.Size([2332, 512])
computing unormed situation
tensor(0.1324, device='cuda:1') tensor(0.0546, device='cuda:1')
computing normed situation
tensor(0.1767, device='cuda:1') tensor(0.0734, device='cuda:1')
un-norm situation:
learned: map: 0.1323915272951126, prec: 0.05456666648387909
norm situation:
learned: map: 0.176717147231102, prec: 0.07343999296426773
Epoch Time:4m17s lr:0.0005000 mAP:0.1773 prec:0.1421


***Training and Validation complete***
================Parameters Settings=================
Parameters:	Namespace(num_shots=4, position='all', params=['q', 'k', 'v'], r=6, alpha=1, dropout_rate=0.25, log_name='reviewer_1', encoder='both', visual='VPT', vptNumTokens=4, ctx_init='a photo of a', text='CoOp', textNumTokens=4, optimizer='adam', l2_reg=0.0, epochs=1, lr=0.0005, momentum=0.9, resume_dict=None, code_path='/home/user/Code/DePro', dataset_path='/data/UCDR/data', dataset='DomainNet', is_eccv_split=1, clip_backbone='ViT-B/32', CLS_NUM_TOKENS=300, DOM_NUM_TOKENS=5, debug_mode=0, dropout=0.5, seen_domain='sketch', holdout_domain='quickdraw', gallery_domain='real', include_auxillary_domains=1, udcdr=0, image_size=224, seed=0, batch_size=50, num_workers=6, early_stop=2, log_interval=100)
================Training Settings=================
lr = 0.0005
batch_size = 50
==================================================
Residual Attention Block 0: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 1: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 2: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 3: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 4: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 5: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 6: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 7: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 8: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 9: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 10: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 11: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 0: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 1: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 2: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 3: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 4: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 5: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 6: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 7: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 8: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 9: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 10: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 11: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
======== Context-aware Simulator Learning Setup========
visual.transformer.resblocks.0.attn.q_proj.w_lora_A
visual.transformer.resblocks.0.attn.q_proj.w_lora_B
visual.transformer.resblocks.0.attn.k_proj.w_lora_A
visual.transformer.resblocks.0.attn.k_proj.w_lora_B
visual.transformer.resblocks.0.attn.v_proj.w_lora_A
visual.transformer.resblocks.0.attn.v_proj.w_lora_B
visual.transformer.resblocks.1.attn.q_proj.w_lora_A
visual.transformer.resblocks.1.attn.q_proj.w_lora_B
visual.transformer.resblocks.1.attn.k_proj.w_lora_A
visual.transformer.resblocks.1.attn.k_proj.w_lora_B
visual.transformer.resblocks.1.attn.v_proj.w_lora_A
visual.transformer.resblocks.1.attn.v_proj.w_lora_B
visual.transformer.resblocks.2.attn.q_proj.w_lora_A
visual.transformer.resblocks.2.attn.q_proj.w_lora_B
visual.transformer.resblocks.2.attn.k_proj.w_lora_A
visual.transformer.resblocks.2.attn.k_proj.w_lora_B
visual.transformer.resblocks.2.attn.v_proj.w_lora_A
visual.transformer.resblocks.2.attn.v_proj.w_lora_B
visual.transformer.resblocks.3.attn.q_proj.w_lora_A
visual.transformer.resblocks.3.attn.q_proj.w_lora_B
visual.transformer.resblocks.3.attn.k_proj.w_lora_A
visual.transformer.resblocks.3.attn.k_proj.w_lora_B
visual.transformer.resblocks.3.attn.v_proj.w_lora_A
visual.transformer.resblocks.3.attn.v_proj.w_lora_B
visual.transformer.resblocks.4.attn.q_proj.w_lora_A
visual.transformer.resblocks.4.attn.q_proj.w_lora_B
visual.transformer.resblocks.4.attn.k_proj.w_lora_A
visual.transformer.resblocks.4.attn.k_proj.w_lora_B
visual.transformer.resblocks.4.attn.v_proj.w_lora_A
visual.transformer.resblocks.4.attn.v_proj.w_lora_B
visual.transformer.resblocks.5.attn.q_proj.w_lora_A
visual.transformer.resblocks.5.attn.q_proj.w_lora_B
visual.transformer.resblocks.5.attn.k_proj.w_lora_A
visual.transformer.resblocks.5.attn.k_proj.w_lora_B
visual.transformer.resblocks.5.attn.v_proj.w_lora_A
visual.transformer.resblocks.5.attn.v_proj.w_lora_B
visual.transformer.resblocks.6.attn.q_proj.w_lora_A
visual.transformer.resblocks.6.attn.q_proj.w_lora_B
visual.transformer.resblocks.6.attn.k_proj.w_lora_A
visual.transformer.resblocks.6.attn.k_proj.w_lora_B
visual.transformer.resblocks.6.attn.v_proj.w_lora_A
visual.transformer.resblocks.6.attn.v_proj.w_lora_B
visual.transformer.resblocks.7.attn.q_proj.w_lora_A
visual.transformer.resblocks.7.attn.q_proj.w_lora_B
visual.transformer.resblocks.7.attn.k_proj.w_lora_A
visual.transformer.resblocks.7.attn.k_proj.w_lora_B
visual.transformer.resblocks.7.attn.v_proj.w_lora_A
visual.transformer.resblocks.7.attn.v_proj.w_lora_B
visual.transformer.resblocks.8.attn.q_proj.w_lora_A
visual.transformer.resblocks.8.attn.q_proj.w_lora_B
visual.transformer.resblocks.8.attn.k_proj.w_lora_A
visual.transformer.resblocks.8.attn.k_proj.w_lora_B
visual.transformer.resblocks.8.attn.v_proj.w_lora_A
visual.transformer.resblocks.8.attn.v_proj.w_lora_B
visual.transformer.resblocks.9.attn.q_proj.w_lora_A
visual.transformer.resblocks.9.attn.q_proj.w_lora_B
visual.transformer.resblocks.9.attn.k_proj.w_lora_A
visual.transformer.resblocks.9.attn.k_proj.w_lora_B
visual.transformer.resblocks.9.attn.v_proj.w_lora_A
visual.transformer.resblocks.9.attn.v_proj.w_lora_B
visual.transformer.resblocks.10.attn.q_proj.w_lora_A
visual.transformer.resblocks.10.attn.q_proj.w_lora_B
visual.transformer.resblocks.10.attn.k_proj.w_lora_A
visual.transformer.resblocks.10.attn.k_proj.w_lora_B
visual.transformer.resblocks.10.attn.v_proj.w_lora_A
visual.transformer.resblocks.10.attn.v_proj.w_lora_B
visual.transformer.resblocks.11.attn.q_proj.w_lora_A
visual.transformer.resblocks.11.attn.q_proj.w_lora_B
visual.transformer.resblocks.11.attn.k_proj.w_lora_A
visual.transformer.resblocks.11.attn.k_proj.w_lora_B
visual.transformer.resblocks.11.attn.v_proj.w_lora_A
visual.transformer.resblocks.11.attn.v_proj.w_lora_B
transformer.resblocks.0.attn.q_proj.w_lora_A
transformer.resblocks.0.attn.q_proj.w_lora_B
transformer.resblocks.0.attn.k_proj.w_lora_A
transformer.resblocks.0.attn.k_proj.w_lora_B
transformer.resblocks.0.attn.v_proj.w_lora_A
transformer.resblocks.0.attn.v_proj.w_lora_B
transformer.resblocks.1.attn.q_proj.w_lora_A
transformer.resblocks.1.attn.q_proj.w_lora_B
transformer.resblocks.1.attn.k_proj.w_lora_A
transformer.resblocks.1.attn.k_proj.w_lora_B
transformer.resblocks.1.attn.v_proj.w_lora_A
transformer.resblocks.1.attn.v_proj.w_lora_B
transformer.resblocks.2.attn.q_proj.w_lora_A
transformer.resblocks.2.attn.q_proj.w_lora_B
transformer.resblocks.2.attn.k_proj.w_lora_A
transformer.resblocks.2.attn.k_proj.w_lora_B
transformer.resblocks.2.attn.v_proj.w_lora_A
transformer.resblocks.2.attn.v_proj.w_lora_B
transformer.resblocks.3.attn.q_proj.w_lora_A
transformer.resblocks.3.attn.q_proj.w_lora_B
transformer.resblocks.3.attn.k_proj.w_lora_A
transformer.resblocks.3.attn.k_proj.w_lora_B
transformer.resblocks.3.attn.v_proj.w_lora_A
transformer.resblocks.3.attn.v_proj.w_lora_B
transformer.resblocks.4.attn.q_proj.w_lora_A
transformer.resblocks.4.attn.q_proj.w_lora_B
transformer.resblocks.4.attn.k_proj.w_lora_A
transformer.resblocks.4.attn.k_proj.w_lora_B
transformer.resblocks.4.attn.v_proj.w_lora_A
transformer.resblocks.4.attn.v_proj.w_lora_B
transformer.resblocks.5.attn.q_proj.w_lora_A
transformer.resblocks.5.attn.q_proj.w_lora_B
transformer.resblocks.5.attn.k_proj.w_lora_A
transformer.resblocks.5.attn.k_proj.w_lora_B
transformer.resblocks.5.attn.v_proj.w_lora_A
transformer.resblocks.5.attn.v_proj.w_lora_B
transformer.resblocks.6.attn.q_proj.w_lora_A
transformer.resblocks.6.attn.q_proj.w_lora_B
transformer.resblocks.6.attn.k_proj.w_lora_A
transformer.resblocks.6.attn.k_proj.w_lora_B
transformer.resblocks.6.attn.v_proj.w_lora_A
transformer.resblocks.6.attn.v_proj.w_lora_B
transformer.resblocks.7.attn.q_proj.w_lora_A
transformer.resblocks.7.attn.q_proj.w_lora_B
transformer.resblocks.7.attn.k_proj.w_lora_A
transformer.resblocks.7.attn.k_proj.w_lora_B
transformer.resblocks.7.attn.v_proj.w_lora_A
transformer.resblocks.7.attn.v_proj.w_lora_B
transformer.resblocks.8.attn.q_proj.w_lora_A
transformer.resblocks.8.attn.q_proj.w_lora_B
transformer.resblocks.8.attn.k_proj.w_lora_A
transformer.resblocks.8.attn.k_proj.w_lora_B
transformer.resblocks.8.attn.v_proj.w_lora_A
transformer.resblocks.8.attn.v_proj.w_lora_B
transformer.resblocks.9.attn.q_proj.w_lora_A
transformer.resblocks.9.attn.q_proj.w_lora_B
transformer.resblocks.9.attn.k_proj.w_lora_A
transformer.resblocks.9.attn.k_proj.w_lora_B
transformer.resblocks.9.attn.v_proj.w_lora_A
transformer.resblocks.9.attn.v_proj.w_lora_B
transformer.resblocks.10.attn.q_proj.w_lora_A
transformer.resblocks.10.attn.q_proj.w_lora_B
transformer.resblocks.10.attn.k_proj.w_lora_A
transformer.resblocks.10.attn.k_proj.w_lora_B
transformer.resblocks.10.attn.v_proj.w_lora_A
transformer.resblocks.10.attn.v_proj.w_lora_B
transformer.resblocks.11.attn.q_proj.w_lora_A
transformer.resblocks.11.attn.q_proj.w_lora_B
transformer.resblocks.11.attn.k_proj.w_lora_A
transformer.resblocks.11.attn.k_proj.w_lora_B
transformer.resblocks.11.attn.v_proj.w_lora_A
transformer.resblocks.11.attn.v_proj.w_lora_B
tot=151277313, train = 552960
===============================================
[Train] Epoch: [1/1][100/120]  Time 0.166 (0.177)  cls 2.2192 (2.3620)  contrastive 0.7840 (6.1196)  loss 3.0031 (8.4816)  
epoch = [1/1]loss = {'net': 7.640452934304873, 'acc': 0.542}

***Validation***
udcdr == 0
Query:quickdraw; Gallery:real; Generalized:0

Query Emb Dim:torch.Size([22500, 512]); Gallery Emb Dim:torch.Size([24387, 512])
computing unormed situation
tensor(0.1300, device='cuda:1') tensor(0.0820, device='cuda:1')
computing normed situation
tensor(0.1606, device='cuda:1') tensor(0.1103, device='cuda:1')
un-norm situation:
learned: map: 0.13004040718078613, prec: 0.08201822638511658
norm situation:
learned: map: 0.160615473985672, prec: 0.11027155071496964
Query:quickdraw; Gallery:real; Generalized:1

Query Emb Dim:torch.Size([22500, 512]); Gallery Emb Dim:torch.Size([36600, 512])
computing unormed situation
tensor(0.1088, device='cuda:1') tensor(0.0602, device='cuda:1')
computing normed situation
tensor(0.1316, device='cuda:1') tensor(0.0834, device='cuda:1')
un-norm situation:
learned: map: 0.10883617401123047, prec: 0.060192666947841644
norm situation:
learned: map: 0.13164173066616058, prec: 0.08338688313961029
udcdr == 1

Query Emb Dim:torch.Size([2250, 512]); Gallery Emb Dim:torch.Size([2332, 512])
computing unormed situation
tensor(0.1085, device='cuda:1') tensor(0.0414, device='cuda:1')
computing normed situation
tensor(0.1291, device='cuda:1') tensor(0.0508, device='cuda:1')
un-norm situation:
learned: map: 0.10852359980344772, prec: 0.04135555401444435
norm situation:
learned: map: 0.12912282347679138, prec: 0.05084666609764099
Epoch Time:4m27s lr:0.0005000 mAP:0.1316 prec:0.0834


***Training and Validation complete***
================Parameters Settings=================
Parameters:	Namespace(num_shots=8, position='all', params=['q', 'k', 'v'], r=6, alpha=1, dropout_rate=0.25, log_name='reviewer_1', encoder='both', visual='VPT', vptNumTokens=4, ctx_init='a photo of a', text='CoOp', textNumTokens=4, optimizer='adam', l2_reg=0.0, epochs=1, lr=0.0005, momentum=0.9, resume_dict=None, code_path='/home/user/Code/DePro', dataset_path='/data/UCDR/data', dataset='DomainNet', is_eccv_split=1, clip_backbone='ViT-B/32', CLS_NUM_TOKENS=300, DOM_NUM_TOKENS=5, debug_mode=0, dropout=0.5, seen_domain='sketch', holdout_domain='quickdraw', gallery_domain='real', include_auxillary_domains=1, udcdr=0, image_size=224, seed=0, batch_size=50, num_workers=6, early_stop=2, log_interval=100)
================Training Settings=================
lr = 0.0005
batch_size = 50
==================================================
Residual Attention Block 0: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 1: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 2: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 3: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 4: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 5: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 6: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 7: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 8: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 9: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 10: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 11: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 0: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 1: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 2: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 3: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 4: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 5: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 6: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 7: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 8: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 9: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 10: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 11: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
======== Context-aware Simulator Learning Setup========
visual.transformer.resblocks.0.attn.q_proj.w_lora_A
visual.transformer.resblocks.0.attn.q_proj.w_lora_B
visual.transformer.resblocks.0.attn.k_proj.w_lora_A
visual.transformer.resblocks.0.attn.k_proj.w_lora_B
visual.transformer.resblocks.0.attn.v_proj.w_lora_A
visual.transformer.resblocks.0.attn.v_proj.w_lora_B
visual.transformer.resblocks.1.attn.q_proj.w_lora_A
visual.transformer.resblocks.1.attn.q_proj.w_lora_B
visual.transformer.resblocks.1.attn.k_proj.w_lora_A
visual.transformer.resblocks.1.attn.k_proj.w_lora_B
visual.transformer.resblocks.1.attn.v_proj.w_lora_A
visual.transformer.resblocks.1.attn.v_proj.w_lora_B
visual.transformer.resblocks.2.attn.q_proj.w_lora_A
visual.transformer.resblocks.2.attn.q_proj.w_lora_B
visual.transformer.resblocks.2.attn.k_proj.w_lora_A
visual.transformer.resblocks.2.attn.k_proj.w_lora_B
visual.transformer.resblocks.2.attn.v_proj.w_lora_A
visual.transformer.resblocks.2.attn.v_proj.w_lora_B
visual.transformer.resblocks.3.attn.q_proj.w_lora_A
visual.transformer.resblocks.3.attn.q_proj.w_lora_B
visual.transformer.resblocks.3.attn.k_proj.w_lora_A
visual.transformer.resblocks.3.attn.k_proj.w_lora_B
visual.transformer.resblocks.3.attn.v_proj.w_lora_A
visual.transformer.resblocks.3.attn.v_proj.w_lora_B
visual.transformer.resblocks.4.attn.q_proj.w_lora_A
visual.transformer.resblocks.4.attn.q_proj.w_lora_B
visual.transformer.resblocks.4.attn.k_proj.w_lora_A
visual.transformer.resblocks.4.attn.k_proj.w_lora_B
visual.transformer.resblocks.4.attn.v_proj.w_lora_A
visual.transformer.resblocks.4.attn.v_proj.w_lora_B
visual.transformer.resblocks.5.attn.q_proj.w_lora_A
visual.transformer.resblocks.5.attn.q_proj.w_lora_B
visual.transformer.resblocks.5.attn.k_proj.w_lora_A
visual.transformer.resblocks.5.attn.k_proj.w_lora_B
visual.transformer.resblocks.5.attn.v_proj.w_lora_A
visual.transformer.resblocks.5.attn.v_proj.w_lora_B
visual.transformer.resblocks.6.attn.q_proj.w_lora_A
visual.transformer.resblocks.6.attn.q_proj.w_lora_B
visual.transformer.resblocks.6.attn.k_proj.w_lora_A
visual.transformer.resblocks.6.attn.k_proj.w_lora_B
visual.transformer.resblocks.6.attn.v_proj.w_lora_A
visual.transformer.resblocks.6.attn.v_proj.w_lora_B
visual.transformer.resblocks.7.attn.q_proj.w_lora_A
visual.transformer.resblocks.7.attn.q_proj.w_lora_B
visual.transformer.resblocks.7.attn.k_proj.w_lora_A
visual.transformer.resblocks.7.attn.k_proj.w_lora_B
visual.transformer.resblocks.7.attn.v_proj.w_lora_A
visual.transformer.resblocks.7.attn.v_proj.w_lora_B
visual.transformer.resblocks.8.attn.q_proj.w_lora_A
visual.transformer.resblocks.8.attn.q_proj.w_lora_B
visual.transformer.resblocks.8.attn.k_proj.w_lora_A
visual.transformer.resblocks.8.attn.k_proj.w_lora_B
visual.transformer.resblocks.8.attn.v_proj.w_lora_A
visual.transformer.resblocks.8.attn.v_proj.w_lora_B
visual.transformer.resblocks.9.attn.q_proj.w_lora_A
visual.transformer.resblocks.9.attn.q_proj.w_lora_B
visual.transformer.resblocks.9.attn.k_proj.w_lora_A
visual.transformer.resblocks.9.attn.k_proj.w_lora_B
visual.transformer.resblocks.9.attn.v_proj.w_lora_A
visual.transformer.resblocks.9.attn.v_proj.w_lora_B
visual.transformer.resblocks.10.attn.q_proj.w_lora_A
visual.transformer.resblocks.10.attn.q_proj.w_lora_B
visual.transformer.resblocks.10.attn.k_proj.w_lora_A
visual.transformer.resblocks.10.attn.k_proj.w_lora_B
visual.transformer.resblocks.10.attn.v_proj.w_lora_A
visual.transformer.resblocks.10.attn.v_proj.w_lora_B
visual.transformer.resblocks.11.attn.q_proj.w_lora_A
visual.transformer.resblocks.11.attn.q_proj.w_lora_B
visual.transformer.resblocks.11.attn.k_proj.w_lora_A
visual.transformer.resblocks.11.attn.k_proj.w_lora_B
visual.transformer.resblocks.11.attn.v_proj.w_lora_A
visual.transformer.resblocks.11.attn.v_proj.w_lora_B
transformer.resblocks.0.attn.q_proj.w_lora_A
transformer.resblocks.0.attn.q_proj.w_lora_B
transformer.resblocks.0.attn.k_proj.w_lora_A
transformer.resblocks.0.attn.k_proj.w_lora_B
transformer.resblocks.0.attn.v_proj.w_lora_A
transformer.resblocks.0.attn.v_proj.w_lora_B
transformer.resblocks.1.attn.q_proj.w_lora_A
transformer.resblocks.1.attn.q_proj.w_lora_B
transformer.resblocks.1.attn.k_proj.w_lora_A
transformer.resblocks.1.attn.k_proj.w_lora_B
transformer.resblocks.1.attn.v_proj.w_lora_A
transformer.resblocks.1.attn.v_proj.w_lora_B
transformer.resblocks.2.attn.q_proj.w_lora_A
transformer.resblocks.2.attn.q_proj.w_lora_B
transformer.resblocks.2.attn.k_proj.w_lora_A
transformer.resblocks.2.attn.k_proj.w_lora_B
transformer.resblocks.2.attn.v_proj.w_lora_A
transformer.resblocks.2.attn.v_proj.w_lora_B
transformer.resblocks.3.attn.q_proj.w_lora_A
transformer.resblocks.3.attn.q_proj.w_lora_B
transformer.resblocks.3.attn.k_proj.w_lora_A
transformer.resblocks.3.attn.k_proj.w_lora_B
transformer.resblocks.3.attn.v_proj.w_lora_A
transformer.resblocks.3.attn.v_proj.w_lora_B
transformer.resblocks.4.attn.q_proj.w_lora_A
transformer.resblocks.4.attn.q_proj.w_lora_B
transformer.resblocks.4.attn.k_proj.w_lora_A
transformer.resblocks.4.attn.k_proj.w_lora_B
transformer.resblocks.4.attn.v_proj.w_lora_A
transformer.resblocks.4.attn.v_proj.w_lora_B
transformer.resblocks.5.attn.q_proj.w_lora_A
transformer.resblocks.5.attn.q_proj.w_lora_B
transformer.resblocks.5.attn.k_proj.w_lora_A
transformer.resblocks.5.attn.k_proj.w_lora_B
transformer.resblocks.5.attn.v_proj.w_lora_A
transformer.resblocks.5.attn.v_proj.w_lora_B
transformer.resblocks.6.attn.q_proj.w_lora_A
transformer.resblocks.6.attn.q_proj.w_lora_B
transformer.resblocks.6.attn.k_proj.w_lora_A
transformer.resblocks.6.attn.k_proj.w_lora_B
transformer.resblocks.6.attn.v_proj.w_lora_A
transformer.resblocks.6.attn.v_proj.w_lora_B
transformer.resblocks.7.attn.q_proj.w_lora_A
transformer.resblocks.7.attn.q_proj.w_lora_B
transformer.resblocks.7.attn.k_proj.w_lora_A
transformer.resblocks.7.attn.k_proj.w_lora_B
transformer.resblocks.7.attn.v_proj.w_lora_A
transformer.resblocks.7.attn.v_proj.w_lora_B
transformer.resblocks.8.attn.q_proj.w_lora_A
transformer.resblocks.8.attn.q_proj.w_lora_B
transformer.resblocks.8.attn.k_proj.w_lora_A
transformer.resblocks.8.attn.k_proj.w_lora_B
transformer.resblocks.8.attn.v_proj.w_lora_A
transformer.resblocks.8.attn.v_proj.w_lora_B
transformer.resblocks.9.attn.q_proj.w_lora_A
transformer.resblocks.9.attn.q_proj.w_lora_B
transformer.resblocks.9.attn.k_proj.w_lora_A
transformer.resblocks.9.attn.k_proj.w_lora_B
transformer.resblocks.9.attn.v_proj.w_lora_A
transformer.resblocks.9.attn.v_proj.w_lora_B
transformer.resblocks.10.attn.q_proj.w_lora_A
transformer.resblocks.10.attn.q_proj.w_lora_B
transformer.resblocks.10.attn.k_proj.w_lora_A
transformer.resblocks.10.attn.k_proj.w_lora_B
transformer.resblocks.10.attn.v_proj.w_lora_A
transformer.resblocks.10.attn.v_proj.w_lora_B
transformer.resblocks.11.attn.q_proj.w_lora_A
transformer.resblocks.11.attn.q_proj.w_lora_B
transformer.resblocks.11.attn.k_proj.w_lora_A
transformer.resblocks.11.attn.k_proj.w_lora_B
transformer.resblocks.11.attn.v_proj.w_lora_A
transformer.resblocks.11.attn.v_proj.w_lora_B
tot=151277313, train = 552960
===============================================
[Train] Epoch: [1/1][100/240]  Time 0.165 (0.176)  cls 1.8039 (2.4893)  contrastive 1.0635 (6.0600)  loss 2.8674 (8.5493)  
[Train] Epoch: [1/1][200/240]  Time 0.166 (0.171)  cls 1.9370 (2.3466)  contrastive 1.0746 (3.5582)  loss 3.0116 (5.9048)  
epoch = [1/1]loss = {'net': 5.40208107928435, 'acc': 0.54725}

***Validation***
udcdr == 0
Query:quickdraw; Gallery:real; Generalized:0

Query Emb Dim:torch.Size([22500, 512]); Gallery Emb Dim:torch.Size([24387, 512])
computing unormed situation
tensor(0.1678, device='cuda:1') tensor(0.1157, device='cuda:1')
computing normed situation
tensor(0.1977, device='cuda:1') tensor(0.1489, device='cuda:1')
un-norm situation:
learned: map: 0.167774498462677, prec: 0.11571088433265686
norm situation:
learned: map: 0.19766084849834442, prec: 0.14885132014751434
Query:quickdraw; Gallery:real; Generalized:1

Query Emb Dim:torch.Size([22500, 512]); Gallery Emb Dim:torch.Size([36600, 512])
computing unormed situation
tensor(0.1403, device='cuda:1') tensor(0.0873, device='cuda:1')
computing normed situation
tensor(0.1621, device='cuda:1') tensor(0.1159, device='cuda:1')
un-norm situation:
learned: map: 0.14027495682239532, prec: 0.08732733130455017
norm situation:
learned: map: 0.16212476789951324, prec: 0.11594533920288086
udcdr == 1

Query Emb Dim:torch.Size([2250, 512]); Gallery Emb Dim:torch.Size([2332, 512])
computing unormed situation
tensor(0.1338, device='cuda:1') tensor(0.0509, device='cuda:1')
computing normed situation
tensor(0.1615, device='cuda:1') tensor(0.0625, device='cuda:1')
un-norm situation:
learned: map: 0.13379161059856415, prec: 0.050928886979818344
norm situation:
learned: map: 0.16145724058151245, prec: 0.06254222244024277
Epoch Time:4m47s lr:0.0005000 mAP:0.1621 prec:0.1159


***Training and Validation complete***
================Parameters Settings=================
Parameters:	Namespace(num_shots=16, position='all', params=['q', 'k', 'v'], r=6, alpha=1, dropout_rate=0.25, log_name='reviewer_1', encoder='both', visual='VPT', vptNumTokens=4, ctx_init='a photo of a', text='CoOp', textNumTokens=4, optimizer='adam', l2_reg=0.0, epochs=1, lr=0.0005, momentum=0.9, resume_dict=None, code_path='/home/user/Code/DePro', dataset_path='/data/UCDR/data', dataset='DomainNet', is_eccv_split=1, clip_backbone='ViT-B/32', CLS_NUM_TOKENS=300, DOM_NUM_TOKENS=5, debug_mode=0, dropout=0.5, seen_domain='sketch', holdout_domain='quickdraw', gallery_domain='real', include_auxillary_domains=1, udcdr=0, image_size=224, seed=0, batch_size=50, num_workers=6, early_stop=2, log_interval=100)
================Training Settings=================
lr = 0.0005
batch_size = 50
==================================================
Residual Attention Block 0: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 1: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 2: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 3: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 4: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 5: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 6: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 7: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 8: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 9: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 10: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 11: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 0: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 1: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 2: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 3: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 4: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 5: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 6: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 7: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 8: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 9: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 10: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 11: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
======== Context-aware Simulator Learning Setup========
visual.transformer.resblocks.0.attn.q_proj.w_lora_A
visual.transformer.resblocks.0.attn.q_proj.w_lora_B
visual.transformer.resblocks.0.attn.k_proj.w_lora_A
visual.transformer.resblocks.0.attn.k_proj.w_lora_B
visual.transformer.resblocks.0.attn.v_proj.w_lora_A
visual.transformer.resblocks.0.attn.v_proj.w_lora_B
visual.transformer.resblocks.1.attn.q_proj.w_lora_A
visual.transformer.resblocks.1.attn.q_proj.w_lora_B
visual.transformer.resblocks.1.attn.k_proj.w_lora_A
visual.transformer.resblocks.1.attn.k_proj.w_lora_B
visual.transformer.resblocks.1.attn.v_proj.w_lora_A
visual.transformer.resblocks.1.attn.v_proj.w_lora_B
visual.transformer.resblocks.2.attn.q_proj.w_lora_A
visual.transformer.resblocks.2.attn.q_proj.w_lora_B
visual.transformer.resblocks.2.attn.k_proj.w_lora_A
visual.transformer.resblocks.2.attn.k_proj.w_lora_B
visual.transformer.resblocks.2.attn.v_proj.w_lora_A
visual.transformer.resblocks.2.attn.v_proj.w_lora_B
visual.transformer.resblocks.3.attn.q_proj.w_lora_A
visual.transformer.resblocks.3.attn.q_proj.w_lora_B
visual.transformer.resblocks.3.attn.k_proj.w_lora_A
visual.transformer.resblocks.3.attn.k_proj.w_lora_B
visual.transformer.resblocks.3.attn.v_proj.w_lora_A
visual.transformer.resblocks.3.attn.v_proj.w_lora_B
visual.transformer.resblocks.4.attn.q_proj.w_lora_A
visual.transformer.resblocks.4.attn.q_proj.w_lora_B
visual.transformer.resblocks.4.attn.k_proj.w_lora_A
visual.transformer.resblocks.4.attn.k_proj.w_lora_B
visual.transformer.resblocks.4.attn.v_proj.w_lora_A
visual.transformer.resblocks.4.attn.v_proj.w_lora_B
visual.transformer.resblocks.5.attn.q_proj.w_lora_A
visual.transformer.resblocks.5.attn.q_proj.w_lora_B
visual.transformer.resblocks.5.attn.k_proj.w_lora_A
visual.transformer.resblocks.5.attn.k_proj.w_lora_B
visual.transformer.resblocks.5.attn.v_proj.w_lora_A
visual.transformer.resblocks.5.attn.v_proj.w_lora_B
visual.transformer.resblocks.6.attn.q_proj.w_lora_A
visual.transformer.resblocks.6.attn.q_proj.w_lora_B
visual.transformer.resblocks.6.attn.k_proj.w_lora_A
visual.transformer.resblocks.6.attn.k_proj.w_lora_B
visual.transformer.resblocks.6.attn.v_proj.w_lora_A
visual.transformer.resblocks.6.attn.v_proj.w_lora_B
visual.transformer.resblocks.7.attn.q_proj.w_lora_A
visual.transformer.resblocks.7.attn.q_proj.w_lora_B
visual.transformer.resblocks.7.attn.k_proj.w_lora_A
visual.transformer.resblocks.7.attn.k_proj.w_lora_B
visual.transformer.resblocks.7.attn.v_proj.w_lora_A
visual.transformer.resblocks.7.attn.v_proj.w_lora_B
visual.transformer.resblocks.8.attn.q_proj.w_lora_A
visual.transformer.resblocks.8.attn.q_proj.w_lora_B
visual.transformer.resblocks.8.attn.k_proj.w_lora_A
visual.transformer.resblocks.8.attn.k_proj.w_lora_B
visual.transformer.resblocks.8.attn.v_proj.w_lora_A
visual.transformer.resblocks.8.attn.v_proj.w_lora_B
visual.transformer.resblocks.9.attn.q_proj.w_lora_A
visual.transformer.resblocks.9.attn.q_proj.w_lora_B
visual.transformer.resblocks.9.attn.k_proj.w_lora_A
visual.transformer.resblocks.9.attn.k_proj.w_lora_B
visual.transformer.resblocks.9.attn.v_proj.w_lora_A
visual.transformer.resblocks.9.attn.v_proj.w_lora_B
visual.transformer.resblocks.10.attn.q_proj.w_lora_A
visual.transformer.resblocks.10.attn.q_proj.w_lora_B
visual.transformer.resblocks.10.attn.k_proj.w_lora_A
visual.transformer.resblocks.10.attn.k_proj.w_lora_B
visual.transformer.resblocks.10.attn.v_proj.w_lora_A
visual.transformer.resblocks.10.attn.v_proj.w_lora_B
visual.transformer.resblocks.11.attn.q_proj.w_lora_A
visual.transformer.resblocks.11.attn.q_proj.w_lora_B
visual.transformer.resblocks.11.attn.k_proj.w_lora_A
visual.transformer.resblocks.11.attn.k_proj.w_lora_B
visual.transformer.resblocks.11.attn.v_proj.w_lora_A
visual.transformer.resblocks.11.attn.v_proj.w_lora_B
transformer.resblocks.0.attn.q_proj.w_lora_A
transformer.resblocks.0.attn.q_proj.w_lora_B
transformer.resblocks.0.attn.k_proj.w_lora_A
transformer.resblocks.0.attn.k_proj.w_lora_B
transformer.resblocks.0.attn.v_proj.w_lora_A
transformer.resblocks.0.attn.v_proj.w_lora_B
transformer.resblocks.1.attn.q_proj.w_lora_A
transformer.resblocks.1.attn.q_proj.w_lora_B
transformer.resblocks.1.attn.k_proj.w_lora_A
transformer.resblocks.1.attn.k_proj.w_lora_B
transformer.resblocks.1.attn.v_proj.w_lora_A
transformer.resblocks.1.attn.v_proj.w_lora_B
transformer.resblocks.2.attn.q_proj.w_lora_A
transformer.resblocks.2.attn.q_proj.w_lora_B
transformer.resblocks.2.attn.k_proj.w_lora_A
transformer.resblocks.2.attn.k_proj.w_lora_B
transformer.resblocks.2.attn.v_proj.w_lora_A
transformer.resblocks.2.attn.v_proj.w_lora_B
transformer.resblocks.3.attn.q_proj.w_lora_A
transformer.resblocks.3.attn.q_proj.w_lora_B
transformer.resblocks.3.attn.k_proj.w_lora_A
transformer.resblocks.3.attn.k_proj.w_lora_B
transformer.resblocks.3.attn.v_proj.w_lora_A
transformer.resblocks.3.attn.v_proj.w_lora_B
transformer.resblocks.4.attn.q_proj.w_lora_A
transformer.resblocks.4.attn.q_proj.w_lora_B
transformer.resblocks.4.attn.k_proj.w_lora_A
transformer.resblocks.4.attn.k_proj.w_lora_B
transformer.resblocks.4.attn.v_proj.w_lora_A
transformer.resblocks.4.attn.v_proj.w_lora_B
transformer.resblocks.5.attn.q_proj.w_lora_A
transformer.resblocks.5.attn.q_proj.w_lora_B
transformer.resblocks.5.attn.k_proj.w_lora_A
transformer.resblocks.5.attn.k_proj.w_lora_B
transformer.resblocks.5.attn.v_proj.w_lora_A
transformer.resblocks.5.attn.v_proj.w_lora_B
transformer.resblocks.6.attn.q_proj.w_lora_A
transformer.resblocks.6.attn.q_proj.w_lora_B
transformer.resblocks.6.attn.k_proj.w_lora_A
transformer.resblocks.6.attn.k_proj.w_lora_B
transformer.resblocks.6.attn.v_proj.w_lora_A
transformer.resblocks.6.attn.v_proj.w_lora_B
transformer.resblocks.7.attn.q_proj.w_lora_A
transformer.resblocks.7.attn.q_proj.w_lora_B
transformer.resblocks.7.attn.k_proj.w_lora_A
transformer.resblocks.7.attn.k_proj.w_lora_B
transformer.resblocks.7.attn.v_proj.w_lora_A
transformer.resblocks.7.attn.v_proj.w_lora_B
transformer.resblocks.8.attn.q_proj.w_lora_A
transformer.resblocks.8.attn.q_proj.w_lora_B
transformer.resblocks.8.attn.k_proj.w_lora_A
transformer.resblocks.8.attn.k_proj.w_lora_B
transformer.resblocks.8.attn.v_proj.w_lora_A
transformer.resblocks.8.attn.v_proj.w_lora_B
transformer.resblocks.9.attn.q_proj.w_lora_A
transformer.resblocks.9.attn.q_proj.w_lora_B
transformer.resblocks.9.attn.k_proj.w_lora_A
transformer.resblocks.9.attn.k_proj.w_lora_B
transformer.resblocks.9.attn.v_proj.w_lora_A
transformer.resblocks.9.attn.v_proj.w_lora_B
transformer.resblocks.10.attn.q_proj.w_lora_A
transformer.resblocks.10.attn.q_proj.w_lora_B
transformer.resblocks.10.attn.k_proj.w_lora_A
transformer.resblocks.10.attn.k_proj.w_lora_B
transformer.resblocks.10.attn.v_proj.w_lora_A
transformer.resblocks.10.attn.v_proj.w_lora_B
transformer.resblocks.11.attn.q_proj.w_lora_A
transformer.resblocks.11.attn.q_proj.w_lora_B
transformer.resblocks.11.attn.k_proj.w_lora_A
transformer.resblocks.11.attn.k_proj.w_lora_B
transformer.resblocks.11.attn.v_proj.w_lora_A
transformer.resblocks.11.attn.v_proj.w_lora_B
tot=151277313, train = 552960
===============================================
[Train] Epoch: [1/1][100/473]  Time 0.165 (0.176)  cls 2.6408 (2.3576)  contrastive 1.6437 (6.0749)  loss 4.2845 (8.4325)  
[Train] Epoch: [1/1][200/473]  Time 0.165 (0.171)  cls 2.4151 (2.2537)  contrastive 1.4708 (3.5441)  loss 3.8860 (5.7978)  
[Train] Epoch: [1/1][300/473]  Time 0.166 (0.169)  cls 1.9140 (2.1944)  contrastive 0.7487 (2.6488)  loss 2.6627 (4.8433)  
[Train] Epoch: [1/1][400/473]  Time 0.165 (0.168)  cls 1.9913 (2.1579)  contrastive 0.5260 (2.1984)  loss 2.5173 (4.3563)  
epoch = [1/1]loss = {'net': 4.122620189653839, 'acc': 0.5708544839255499}

***Validation***
udcdr == 0
Query:quickdraw; Gallery:real; Generalized:0

Query Emb Dim:torch.Size([22500, 512]); Gallery Emb Dim:torch.Size([24387, 512])
computing unormed situation
tensor(0.1675, device='cuda:1') tensor(0.1192, device='cuda:1')
computing normed situation
tensor(0.1919, device='cuda:1') tensor(0.1469, device='cuda:1')
un-norm situation:
learned: map: 0.16748349368572235, prec: 0.1192459985613823
norm situation:
learned: map: 0.19192326068878174, prec: 0.1468626707792282
Query:quickdraw; Gallery:real; Generalized:1

Query Emb Dim:torch.Size([22500, 512]); Gallery Emb Dim:torch.Size([36600, 512])
computing unormed situation
tensor(0.1411, device='cuda:1') tensor(0.0895, device='cuda:1')
computing normed situation
tensor(0.1589, device='cuda:1') tensor(0.1127, device='cuda:1')
un-norm situation:
learned: map: 0.14109963178634644, prec: 0.08947200328111649
norm situation:
learned: map: 0.158866286277771, prec: 0.11271088570356369
udcdr == 1

Query Emb Dim:torch.Size([2250, 512]); Gallery Emb Dim:torch.Size([2332, 512])
computing unormed situation
tensor(0.1447, device='cuda:1') tensor(0.0536, device='cuda:1')
computing normed situation
tensor(0.1658, device='cuda:1') tensor(0.0640, device='cuda:1')
un-norm situation:
learned: map: 0.1447114646434784, prec: 0.05361555516719818
norm situation:
learned: map: 0.16579118371009827, prec: 0.06400000303983688
Epoch Time:5m26s lr:0.0005000 mAP:0.1589 prec:0.1127


***Training and Validation complete***
